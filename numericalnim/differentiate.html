<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<!--  This file is generated by Nim. -->
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />

<meta name="viewport" content="width=device-width, initial-scale=1.0">

<!-- Favicon -->
<link rel="shortcut icon" href="data:image/x-icon;base64,AAABAAEAEBAAAAEAIABoBAAAFgAAACgAAAAQAAAAIAAAAAEAIAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD///8A////AP///wD///8A////AP///wD///8A////AP///wD///8A////AP///wD///8A////AP///wD///8A////AP///wD///8A////AP///wD///8A////AAAAAAUAAAAF////AP///wD///8A////AP///wD///8A////AP///wD///8A////AAAAAAIAAABbAAAAlQAAAKIAAACbAAAAmwAAAKIAAACVAAAAWwAAAAL///8A////AP///wD///8A////AAAAABQAAADAAAAAYwAAAA3///8A////AP///wD///8AAAAADQAAAGMAAADAAAAAFP///wD///8A////AP///wAAAACdAAAAOv///wD///8A////AP///wD///8A////AP///wD///8AAAAAOgAAAJ3///8A////AP///wAAAAAnAAAAcP///wAAAAAoAAAASv///wD///8A////AP///wAAAABKAAAAKP///wAAAABwAAAAJ////wD///8AAAAAgQAAABwAAACIAAAAkAAAAJMAAACtAAAAFQAAABUAAACtAAAAkwAAAJAAAACIAAAAHAAAAIH///8A////AAAAAKQAAACrAAAAaP///wD///8AAAAARQAAANIAAADSAAAARf///wD///8AAAAAaAAAAKsAAACk////AAAAADMAAACcAAAAnQAAABj///8A////AP///wAAAAAYAAAAGP///wD///8A////AAAAABgAAACdAAAAnAAAADMAAAB1AAAAwwAAAP8AAADpAAAAsQAAAE4AAAAb////AP///wAAAAAbAAAATgAAALEAAADpAAAA/wAAAMMAAAB1AAAAtwAAAOkAAAD/AAAA/wAAAP8AAADvAAAA3gAAAN4AAADeAAAA3gAAAO8AAAD/AAAA/wAAAP8AAADpAAAAtwAAAGUAAAA/AAAA3wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAADfAAAAPwAAAGX///8A////AAAAAEgAAADtAAAAvwAAAL0AAADGAAAA7wAAAO8AAADGAAAAvQAAAL8AAADtAAAASP///wD///8A////AP///wD///8AAAAAO////wD///8A////AAAAAIcAAACH////AP///wD///8AAAAAO////wD///8A////AP///wD///8A////AP///wD///8A////AP///wD///8A////AP///wD///8A////AP///wD///8A////AP///wD///8A////AP///wD///8A////AP///wD///8A////AP///wD///8A////AP///wD///8A////AP///wD///8A//8AAP//AAD4HwAA7/cAAN/7AAD//wAAoYUAAJ55AACf+QAAh+EAAAAAAADAAwAA4AcAAP5/AAD//wAA//8AAA=="/>
<link rel="icon" type="image/png" sizes="32x32" href="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAABmJLR0QA/wD/AP+gvaeTAAAACXBIWXMAAA3XAAAN1wFCKJt4AAAAB3RJTUUH4QQQEwksSS9ZWwAAAk1JREFUWMPtll2ITVEUx39nn/O7Y5qR8f05wtCUUr6ZIS++8pEnkZInPImneaCQ5METNdOkeFBKUhMPRIkHKfEuUZSUlGlKPN2TrgfncpvmnntnmlEyq1Z7t89/rf9a6+y99oZxGZf/XeIq61EdtgKXgdXA0xrYAvBjOIF1AI9zvjcC74BSpndrJPkBWDScTF8Aa4E3wDlgHbASaANmVqlcCnwHvgDvgVfAJ+AikAAvgfVZwLnSVZHZaOuKoQi3ZOMi4NkYkpe1p4J7A8BpYAD49hfIy/oqG0+hLomiKP2L5L+1ubn5115S+3OAn4EnwBlgMzCjyt6ZAnQCJ4A7wOs88iRJHvw50HoujuPBoCKwHWiosy8MdfZnAdcHk8dxXFJ3VQbQlCTJvRBCGdRbD4M6uc5glpY3eAihpN5S5w12diSEcCCEcKUO4ljdr15T76ur1FDDLIQQ3qv71EdDOe3Kxj3leRXyk+pxdWnFWod6Wt2bY3de3aSuUHcPBVimHs7mK9WrmeOF6lR1o9qnzskh2ar2qm1qizpfXaPeVGdlmGN5pb09qMxz1Xb1kLqgzn1RyH7JUXW52lr5e/Kqi9qpto7V1atuUzfnARrV7jEib1T76gG2qxdGmXyiekkt1GswPTtek0aBfJp6YySGBfWg2tPQ0FAYgf1stUfdmdcjarbYJEniKIq6gY/Aw+zWHAC+p2labGpqiorFYgGYCEzN7oQdQClN07O1/EfDyGgC0ALMBdYAi4FyK+4H3gLPsxfR1zRNi+NP7nH5J+QntnXe5B5mpfQAAAAASUVORK5CYII=">

<!-- Google fonts -->
<link href='https://fonts.googleapis.com/css?family=Lato:400,600,900' rel='stylesheet' type='text/css'/>
<link href='https://fonts.googleapis.com/css?family=Source+Code+Pro:400,500,600' rel='stylesheet' type='text/css'/>

<!-- CSS -->
<title>src/numericalnim/differentiate</title>
<link rel="stylesheet" type="text/css" href="../nimdoc.out.css">

<script type="text/javascript" src="../dochack.js"></script>

<script type="text/javascript">
function main() {
  var pragmaDots = document.getElementsByClassName("pragmadots");
  for (var i = 0; i < pragmaDots.length; i++) {
    pragmaDots[i].onclick = function(event) {
      // Hide tease
      event.target.parentNode.style.display = "none";
      // Show actual
      event.target.parentNode.nextElementSibling.style.display = "inline";
    }
  }

  function switchTheme(e) {
      if (e.target.checked) {
          document.documentElement.setAttribute('data-theme', 'dark');
          localStorage.setItem('theme', 'dark');
      } else {
          document.documentElement.setAttribute('data-theme', 'light');
          localStorage.setItem('theme', 'light');
      }
  }

  const toggleSwitch = document.querySelector('.theme-switch input[type="checkbox"]');
  if (toggleSwitch !== null) {
    toggleSwitch.addEventListener('change', switchTheme, false);
  }

  var currentTheme = localStorage.getItem('theme');
  if (!currentTheme && window.matchMedia('(prefers-color-scheme: dark)').matches) {
    currentTheme = 'dark';
  }
  if (currentTheme) {
    document.documentElement.setAttribute('data-theme', currentTheme);

    if (currentTheme === 'dark' && toggleSwitch !== null) {
      toggleSwitch.checked = true;
    }
  }
}

window.addEventListener('DOMContentLoaded', main);
</script>

</head>
<body>
<div class="document" id="documentId">
  <div class="container">
    <h1 class="title">src/numericalnim/differentiate</h1>
    <div class="row">
  <div class="three columns">
  <div class="theme-switch-wrapper">
    <label class="theme-switch" for="checkbox">
      <input type="checkbox" id="checkbox" />
      <div class="slider round"></div>
    </label>
    &nbsp;&nbsp;&nbsp; <em>Dark Mode</em>
  </div>
  <div id="global-links">
    <ul class="simple">
    <li>
      <a href="../theindex.html">Index</a>
    </li>
    </ul>
  </div>
  <div id="searchInputDiv">
    Search: <input type="text" id="searchInput"
      onkeyup="search()" />
  </div>
  <div>
    Group by:
    <select onchange="groupBy(this.value)">
      <option value="section">Section</option>
      <option value="type">Type</option>
    </select>
  </div>
  <ul class="simple simple-toc" id="toc-list">
<li>
  <a class="reference reference-toplevel" href="#12" id="62">Procs</a>
  <ul class="simple simple-toc-section">
      <ul class="simple nested-toc-section">checkGradient
      <li><a class="reference" href="#checkGradient%2Cproc%28Tensor%5BU%5D%29%2Cproc%28Tensor%5BU%5D%29%2CTensor%5BU%5D%2CT"
    title="checkGradient[U; T: not Tensor](f: proc (x: Tensor[U]): T;
                                fGrad: proc (x: Tensor[U]): Tensor[T];
                                x0: Tensor[U]; tol: T): bool">checkGradient[U; T: not Tensor](f: proc (x: Tensor[U]): T;
                                fGrad: proc (x: Tensor[U]): Tensor[T];
                                x0: Tensor[U]; tol: T): bool</a></li>
  <li><a class="reference" href="#checkGradient%2Cproc%28Tensor%5BU%5D%29%2Cproc%28Tensor%5BU%5D%29%2CTensor%5BU%5D%2CT_2"
    title="checkGradient[U; T](f: proc (x: Tensor[U]): Tensor[T];
                    fGrad: proc (x: Tensor[U]): Tensor[T]; x0: Tensor[U]; tol: T): bool">checkGradient[U; T](f: proc (x: Tensor[U]): Tensor[T];
                    fGrad: proc (x: Tensor[U]): Tensor[T]; x0: Tensor[U]; tol: T): bool</a></li>

  </ul>
  <ul class="simple nested-toc-section">diff1dBackward
      <li><a class="reference" href="#diff1dBackward%2Cproc%28U%29%2CU%2CU"
    title="diff1dBackward[U, T](f: proc (x: U): T; x0: U; h: U = U(0.000001)): T">diff1dBackward[U, T](f: proc (x: U): T; x0: U; h: U = U(0.000001)): T</a></li>

  </ul>
  <ul class="simple nested-toc-section">diff1dCentral
      <li><a class="reference" href="#diff1dCentral%2Cproc%28U%29%2CU%2CU"
    title="diff1dCentral[U, T](f: proc (x: U): T; x0: U; h: U = U(0.000001)): T">diff1dCentral[U, T](f: proc (x: U): T; x0: U; h: U = U(0.000001)): T</a></li>

  </ul>
  <ul class="simple nested-toc-section">diff1dForward
      <li><a class="reference" href="#diff1dForward%2Cproc%28U%29%2CU%2CU"
    title="diff1dForward[U, T](f: proc (x: U): T; x0: U; h: U = U(0.000001)): T">diff1dForward[U, T](f: proc (x: U): T; x0: U; h: U = U(0.000001)): T</a></li>

  </ul>
  <ul class="simple nested-toc-section">mixedDerivative
      <li><a class="reference" href="#mixedDerivative%2Cproc%28Tensor%5BU%5D%29%2CTensor%5BU%5D%2C%2CU"
    title="mixedDerivative[U, T](f: proc (x: Tensor[U]): T; x0: var Tensor[U];
                      indices: (int, int); h: U = U(0.000001)): T">mixedDerivative[U, T](f: proc (x: Tensor[U]): T; x0: var Tensor[U];
                      indices: (int, int); h: U = U(0.000001)): T</a></li>

  </ul>
  <ul class="simple nested-toc-section">secondDiff1dBackward
      <li><a class="reference" href="#secondDiff1dBackward%2Cproc%28U%29%2CU%2CU"
    title="secondDiff1dBackward[U, T](f: proc (x: U): T; x0: U; h: U = U(0.000001)): T">secondDiff1dBackward[U, T](f: proc (x: U): T; x0: U; h: U = U(0.000001)): T</a></li>

  </ul>
  <ul class="simple nested-toc-section">secondDiff1dCentral
      <li><a class="reference" href="#secondDiff1dCentral%2Cproc%28U%29%2CU%2CU"
    title="secondDiff1dCentral[U, T](f: proc (x: U): T; x0: U; h: U = U(0.000001)): T">secondDiff1dCentral[U, T](f: proc (x: U): T; x0: U; h: U = U(0.000001)): T</a></li>

  </ul>
  <ul class="simple nested-toc-section">secondDiff1dForward
      <li><a class="reference" href="#secondDiff1dForward%2Cproc%28U%29%2CU%2CU"
    title="secondDiff1dForward[U, T](f: proc (x: U): T; x0: U; h: U = U(0.000001)): T">secondDiff1dForward[U, T](f: proc (x: U): T; x0: U; h: U = U(0.000001)): T</a></li>

  </ul>
  <ul class="simple nested-toc-section">tensorGradient
      <li><a class="reference" href="#tensorGradient%2Cproc%28Tensor%5BU%5D%29%2CTensor%5BU%5D%2CU%2Cbool_2"
    title="tensorGradient[U, T](f: proc (x: Tensor[U]): Tensor[T]; x0: Tensor[U];
                     h: U = U(0.000001); fastMode: bool = false): Tensor[T]">tensorGradient[U, T](f: proc (x: Tensor[U]): Tensor[T]; x0: Tensor[U];
                     h: U = U(0.000001); fastMode: bool = false): Tensor[T]</a></li>
  <li><a class="reference" href="#tensorGradient%2Cproc%28Tensor%5BU%5D%29%2CTensor%5BU%5D%2CU%2Cbool"
    title="tensorGradient[U; T: not Tensor](f: proc (x: Tensor[U]): T; x0: Tensor[U];
                                 h: U = U(0.000001); fastMode: bool = false): Tensor[
    T]">tensorGradient[U; T: not Tensor](f: proc (x: Tensor[U]): T; x0: Tensor[U];
                                 h: U = U(0.000001); fastMode: bool = false): Tensor[
    T]</a></li>

  </ul>
  <ul class="simple nested-toc-section">tensorHessian
      <li><a class="reference" href="#tensorHessian%2Cproc%28Tensor%5BU%5D%29%2CTensor%5BU%5D%2CU"
    title="tensorHessian[U; T: not Tensor](f: proc (x: Tensor[U]): T; x0: Tensor[U];
                                h: U = U(0.000001)): Tensor[T]">tensorHessian[U; T: not Tensor](f: proc (x: Tensor[U]): T; x0: Tensor[U];
                                h: U = U(0.000001)): Tensor[T]</a></li>

  </ul>
  <ul class="simple nested-toc-section">tensorJacobian
      <li><a class="reference" href="#tensorJacobian%2Cproc%28Tensor%5BU%5D%29%2CTensor%5BU%5D%2CU%2Cbool"
    title="tensorJacobian[U, T](f: proc (x: Tensor[U]): Tensor[T]; x0: Tensor[U];
                     h: U = U(0.000001); fastMode: bool = false): Tensor[T]">tensorJacobian[U, T](f: proc (x: Tensor[U]): Tensor[T]; x0: Tensor[U];
                     h: U = U(0.000001); fastMode: bool = false): Tensor[T]</a></li>

  </ul>

  </ul>
</li>

</ul>

  </div>
  &nbsp;&nbsp;<a
href="https://github.com/SciNim/numericalnim/tree/master/src/numericalnim/differentiate.nim#L1"
class="link-seesrc" target="_blank">Source</a>
&nbsp;&nbsp;<a href="https://github.com/SciNim/numericalnim/edit/devel/src/numericalnim/differentiate.nim#L1" class="link-seesrc" target="_blank" >Edit</a>

  <div class="nine columns" id="content">
  <div id="tocRoot"></div>
  
  <p class="module-desc"></p>
  <div class="section" id="12">
<h1><a class="toc-backref" href="#12">Procs</a></h1>
<dl class="item">
<div id="checkGradient,proc(Tensor[U]),proc(Tensor[U]),Tensor[U],T">
<dt><pre><span class="Keyword">proc</span> <a href="#checkGradient%2Cproc%28Tensor%5BU%5D%29%2Cproc%28Tensor%5BU%5D%29%2CTensor%5BU%5D%2CT"><span class="Identifier">checkGradient</span></a><span class="Other">[</span><span class="Identifier">U</span><span class="Other">;</span> <span class="Identifier">T</span><span class="Other">:</span> <span class="Keyword">not</span> <span class="Identifier">Tensor</span><span class="Other">]</span><span class="Other">(</span><span class="Identifier">f</span><span class="Other">:</span> <span class="Keyword">proc</span> <span class="Other">(</span><span class="Identifier">x</span><span class="Other">:</span> <span class="Identifier">Tensor</span><span class="Other">[</span><span class="Identifier">U</span><span class="Other">]</span><span class="Other">)</span><span class="Other">:</span> <span class="Identifier">T</span><span class="Other">;</span>
                                     <span class="Identifier">fGrad</span><span class="Other">:</span> <span class="Keyword">proc</span> <span class="Other">(</span><span class="Identifier">x</span><span class="Other">:</span> <span class="Identifier">Tensor</span><span class="Other">[</span><span class="Identifier">U</span><span class="Other">]</span><span class="Other">)</span><span class="Other">:</span> <span class="Identifier">Tensor</span><span class="Other">[</span><span class="Identifier">T</span><span class="Other">]</span><span class="Other">;</span>
                                     <span class="Identifier">x0</span><span class="Other">:</span> <span class="Identifier">Tensor</span><span class="Other">[</span><span class="Identifier">U</span><span class="Other">]</span><span class="Other">;</span> <span class="Identifier">tol</span><span class="Other">:</span> <span class="Identifier">T</span><span class="Other">)</span><span class="Other">:</span> <span class="Identifier">bool</span></pre></dt>
<dd>

Checks if the provided gradient function <tt class="docutils literal"><span class="pre"><span class="Identifier">fGrad</span></span></tt> gives the same values as numeric gradient.
&nbsp;&nbsp;<a
href="https://github.com/SciNim/numericalnim/tree/master/src/numericalnim/differentiate.nim#L143"
class="link-seesrc" target="_blank">Source</a>
&nbsp;&nbsp;<a href="https://github.com/SciNim/numericalnim/edit/devel/src/numericalnim/differentiate.nim#L143" class="link-seesrc" target="_blank" >Edit</a>

</dd>
</div>
<div id="checkGradient,proc(Tensor[U]),proc(Tensor[U]),Tensor[U],T_2">
<dt><pre><span class="Keyword">proc</span> <a href="#checkGradient%2Cproc%28Tensor%5BU%5D%29%2Cproc%28Tensor%5BU%5D%29%2CTensor%5BU%5D%2CT_2"><span class="Identifier">checkGradient</span></a><span class="Other">[</span><span class="Identifier">U</span><span class="Other">;</span> <span class="Identifier">T</span><span class="Other">]</span><span class="Other">(</span><span class="Identifier">f</span><span class="Other">:</span> <span class="Keyword">proc</span> <span class="Other">(</span><span class="Identifier">x</span><span class="Other">:</span> <span class="Identifier">Tensor</span><span class="Other">[</span><span class="Identifier">U</span><span class="Other">]</span><span class="Other">)</span><span class="Other">:</span> <span class="Identifier">Tensor</span><span class="Other">[</span><span class="Identifier">T</span><span class="Other">]</span><span class="Other">;</span>
                         <span class="Identifier">fGrad</span><span class="Other">:</span> <span class="Keyword">proc</span> <span class="Other">(</span><span class="Identifier">x</span><span class="Other">:</span> <span class="Identifier">Tensor</span><span class="Other">[</span><span class="Identifier">U</span><span class="Other">]</span><span class="Other">)</span><span class="Other">:</span> <span class="Identifier">Tensor</span><span class="Other">[</span><span class="Identifier">T</span><span class="Other">]</span><span class="Other">;</span> <span class="Identifier">x0</span><span class="Other">:</span> <span class="Identifier">Tensor</span><span class="Other">[</span><span class="Identifier">U</span><span class="Other">]</span><span class="Other">;</span>
                         <span class="Identifier">tol</span><span class="Other">:</span> <span class="Identifier">T</span><span class="Other">)</span><span class="Other">:</span> <span class="Identifier">bool</span></pre></dt>
<dd>

Checks if the provided gradient function <tt class="docutils literal"><span class="pre"><span class="Identifier">fGrad</span></span></tt> gives the same values as numeric gradient.
&nbsp;&nbsp;<a
href="https://github.com/SciNim/numericalnim/tree/master/src/numericalnim/differentiate.nim#L153"
class="link-seesrc" target="_blank">Source</a>
&nbsp;&nbsp;<a href="https://github.com/SciNim/numericalnim/edit/devel/src/numericalnim/differentiate.nim#L153" class="link-seesrc" target="_blank" >Edit</a>

</dd>
</div>
<div id="diff1dBackward,proc(U),U,U">
<dt><pre><span class="Keyword">proc</span> <a href="#diff1dBackward%2Cproc%28U%29%2CU%2CU"><span class="Identifier">diff1dBackward</span></a><span class="Other">[</span><span class="Identifier">U</span><span class="Other">,</span> <span class="Identifier">T</span><span class="Other">]</span><span class="Other">(</span><span class="Identifier">f</span><span class="Other">:</span> <span class="Keyword">proc</span> <span class="Other">(</span><span class="Identifier">x</span><span class="Other">:</span> <span class="Identifier">U</span><span class="Other">)</span><span class="Other">:</span> <span class="Identifier">T</span><span class="Other">;</span> <span class="Identifier">x0</span><span class="Other">:</span> <span class="Identifier">U</span><span class="Other">;</span> <span class="Identifier">h</span><span class="Other">:</span> <span class="Identifier">U</span> <span class="Other">=</span> <span class="Identifier">U</span><span class="Other">(</span><span class="FloatNumber">0.000001</span><span class="Other">)</span><span class="Other">)</span><span class="Other">:</span> <span class="Identifier">T</span></pre></dt>
<dd>

Numerically calculate the derivative of f(x) at x0 using a step size h. Uses backward difference which has accuracy O(h)
&nbsp;&nbsp;<a
href="https://github.com/SciNim/numericalnim/tree/master/src/numericalnim/differentiate.nim#L9"
class="link-seesrc" target="_blank">Source</a>
&nbsp;&nbsp;<a href="https://github.com/SciNim/numericalnim/edit/devel/src/numericalnim/differentiate.nim#L9" class="link-seesrc" target="_blank" >Edit</a>

</dd>
</div>
<div id="diff1dCentral,proc(U),U,U">
<dt><pre><span class="Keyword">proc</span> <a href="#diff1dCentral%2Cproc%28U%29%2CU%2CU"><span class="Identifier">diff1dCentral</span></a><span class="Other">[</span><span class="Identifier">U</span><span class="Other">,</span> <span class="Identifier">T</span><span class="Other">]</span><span class="Other">(</span><span class="Identifier">f</span><span class="Other">:</span> <span class="Keyword">proc</span> <span class="Other">(</span><span class="Identifier">x</span><span class="Other">:</span> <span class="Identifier">U</span><span class="Other">)</span><span class="Other">:</span> <span class="Identifier">T</span><span class="Other">;</span> <span class="Identifier">x0</span><span class="Other">:</span> <span class="Identifier">U</span><span class="Other">;</span> <span class="Identifier">h</span><span class="Other">:</span> <span class="Identifier">U</span> <span class="Other">=</span> <span class="Identifier">U</span><span class="Other">(</span><span class="FloatNumber">0.000001</span><span class="Other">)</span><span class="Other">)</span><span class="Other">:</span> <span class="Identifier">T</span></pre></dt>
<dd>

Numerically calculate the derivative of f(x) at x0 using a step size h. Uses central difference which has accuracy O(h^2)
&nbsp;&nbsp;<a
href="https://github.com/SciNim/numericalnim/tree/master/src/numericalnim/differentiate.nim#L14"
class="link-seesrc" target="_blank">Source</a>
&nbsp;&nbsp;<a href="https://github.com/SciNim/numericalnim/edit/devel/src/numericalnim/differentiate.nim#L14" class="link-seesrc" target="_blank" >Edit</a>

</dd>
</div>
<div id="diff1dForward,proc(U),U,U">
<dt><pre><span class="Keyword">proc</span> <a href="#diff1dForward%2Cproc%28U%29%2CU%2CU"><span class="Identifier">diff1dForward</span></a><span class="Other">[</span><span class="Identifier">U</span><span class="Other">,</span> <span class="Identifier">T</span><span class="Other">]</span><span class="Other">(</span><span class="Identifier">f</span><span class="Other">:</span> <span class="Keyword">proc</span> <span class="Other">(</span><span class="Identifier">x</span><span class="Other">:</span> <span class="Identifier">U</span><span class="Other">)</span><span class="Other">:</span> <span class="Identifier">T</span><span class="Other">;</span> <span class="Identifier">x0</span><span class="Other">:</span> <span class="Identifier">U</span><span class="Other">;</span> <span class="Identifier">h</span><span class="Other">:</span> <span class="Identifier">U</span> <span class="Other">=</span> <span class="Identifier">U</span><span class="Other">(</span><span class="FloatNumber">0.000001</span><span class="Other">)</span><span class="Other">)</span><span class="Other">:</span> <span class="Identifier">T</span></pre></dt>
<dd>

Numerically calculate the derivative of f(x) at x0 using a step size h. Uses forward difference which has accuracy O(h)
&nbsp;&nbsp;<a
href="https://github.com/SciNim/numericalnim/tree/master/src/numericalnim/differentiate.nim#L4"
class="link-seesrc" target="_blank">Source</a>
&nbsp;&nbsp;<a href="https://github.com/SciNim/numericalnim/edit/devel/src/numericalnim/differentiate.nim#L4" class="link-seesrc" target="_blank" >Edit</a>

</dd>
</div>
<div id="mixedDerivative,proc(Tensor[U]),Tensor[U],,U">
<dt><pre><span class="Keyword">proc</span> <a href="#mixedDerivative%2Cproc%28Tensor%5BU%5D%29%2CTensor%5BU%5D%2C%2CU"><span class="Identifier">mixedDerivative</span></a><span class="Other">[</span><span class="Identifier">U</span><span class="Other">,</span> <span class="Identifier">T</span><span class="Other">]</span><span class="Other">(</span><span class="Identifier">f</span><span class="Other">:</span> <span class="Keyword">proc</span> <span class="Other">(</span><span class="Identifier">x</span><span class="Other">:</span> <span class="Identifier">Tensor</span><span class="Other">[</span><span class="Identifier">U</span><span class="Other">]</span><span class="Other">)</span><span class="Other">:</span> <span class="Identifier">T</span><span class="Other">;</span> <span class="Identifier">x0</span><span class="Other">:</span> <span class="Keyword">var</span> <span class="Identifier">Tensor</span><span class="Other">[</span><span class="Identifier">U</span><span class="Other">]</span><span class="Other">;</span>
                           <span class="Identifier">indices</span><span class="Other">:</span> <span class="Other">(</span><span class="Identifier">int</span><span class="Other">,</span> <span class="Identifier">int</span><span class="Other">)</span><span class="Other">;</span> <span class="Identifier">h</span><span class="Other">:</span> <span class="Identifier">U</span> <span class="Other">=</span> <span class="Identifier">U</span><span class="Other">(</span><span class="FloatNumber">0.000001</span><span class="Other">)</span><span class="Other">)</span><span class="Other">:</span> <span class="Identifier">T</span></pre></dt>
<dd>


&nbsp;&nbsp;<a
href="https://github.com/SciNim/numericalnim/tree/master/src/numericalnim/differentiate.nim#L99"
class="link-seesrc" target="_blank">Source</a>
&nbsp;&nbsp;<a href="https://github.com/SciNim/numericalnim/edit/devel/src/numericalnim/differentiate.nim#L99" class="link-seesrc" target="_blank" >Edit</a>

</dd>
</div>
<div id="secondDiff1dBackward,proc(U),U,U">
<dt><pre><span class="Keyword">proc</span> <a href="#secondDiff1dBackward%2Cproc%28U%29%2CU%2CU"><span class="Identifier">secondDiff1dBackward</span></a><span class="Other">[</span><span class="Identifier">U</span><span class="Other">,</span> <span class="Identifier">T</span><span class="Other">]</span><span class="Other">(</span><span class="Identifier">f</span><span class="Other">:</span> <span class="Keyword">proc</span> <span class="Other">(</span><span class="Identifier">x</span><span class="Other">:</span> <span class="Identifier">U</span><span class="Other">)</span><span class="Other">:</span> <span class="Identifier">T</span><span class="Other">;</span> <span class="Identifier">x0</span><span class="Other">:</span> <span class="Identifier">U</span><span class="Other">;</span> <span class="Identifier">h</span><span class="Other">:</span> <span class="Identifier">U</span> <span class="Other">=</span> <span class="Identifier">U</span><span class="Other">(</span><span class="FloatNumber">0.000001</span><span class="Other">)</span><span class="Other">)</span><span class="Other">:</span> <span class="Identifier">T</span></pre></dt>
<dd>

Numerically calculate the second derivative of f(x) at x0 using a step size h.
&nbsp;&nbsp;<a
href="https://github.com/SciNim/numericalnim/tree/master/src/numericalnim/differentiate.nim#L23"
class="link-seesrc" target="_blank">Source</a>
&nbsp;&nbsp;<a href="https://github.com/SciNim/numericalnim/edit/devel/src/numericalnim/differentiate.nim#L23" class="link-seesrc" target="_blank" >Edit</a>

</dd>
</div>
<div id="secondDiff1dCentral,proc(U),U,U">
<dt><pre><span class="Keyword">proc</span> <a href="#secondDiff1dCentral%2Cproc%28U%29%2CU%2CU"><span class="Identifier">secondDiff1dCentral</span></a><span class="Other">[</span><span class="Identifier">U</span><span class="Other">,</span> <span class="Identifier">T</span><span class="Other">]</span><span class="Other">(</span><span class="Identifier">f</span><span class="Other">:</span> <span class="Keyword">proc</span> <span class="Other">(</span><span class="Identifier">x</span><span class="Other">:</span> <span class="Identifier">U</span><span class="Other">)</span><span class="Other">:</span> <span class="Identifier">T</span><span class="Other">;</span> <span class="Identifier">x0</span><span class="Other">:</span> <span class="Identifier">U</span><span class="Other">;</span> <span class="Identifier">h</span><span class="Other">:</span> <span class="Identifier">U</span> <span class="Other">=</span> <span class="Identifier">U</span><span class="Other">(</span><span class="FloatNumber">0.000001</span><span class="Other">)</span><span class="Other">)</span><span class="Other">:</span> <span class="Identifier">T</span></pre></dt>
<dd>

Numerically calculate the second derivative of f(x) at x0 using a step size h. Uses central difference which has accuracy O(h^2)
&nbsp;&nbsp;<a
href="https://github.com/SciNim/numericalnim/tree/master/src/numericalnim/differentiate.nim#L27"
class="link-seesrc" target="_blank">Source</a>
&nbsp;&nbsp;<a href="https://github.com/SciNim/numericalnim/edit/devel/src/numericalnim/differentiate.nim#L27" class="link-seesrc" target="_blank" >Edit</a>

</dd>
</div>
<div id="secondDiff1dForward,proc(U),U,U">
<dt><pre><span class="Keyword">proc</span> <a href="#secondDiff1dForward%2Cproc%28U%29%2CU%2CU"><span class="Identifier">secondDiff1dForward</span></a><span class="Other">[</span><span class="Identifier">U</span><span class="Other">,</span> <span class="Identifier">T</span><span class="Other">]</span><span class="Other">(</span><span class="Identifier">f</span><span class="Other">:</span> <span class="Keyword">proc</span> <span class="Other">(</span><span class="Identifier">x</span><span class="Other">:</span> <span class="Identifier">U</span><span class="Other">)</span><span class="Other">:</span> <span class="Identifier">T</span><span class="Other">;</span> <span class="Identifier">x0</span><span class="Other">:</span> <span class="Identifier">U</span><span class="Other">;</span> <span class="Identifier">h</span><span class="Other">:</span> <span class="Identifier">U</span> <span class="Other">=</span> <span class="Identifier">U</span><span class="Other">(</span><span class="FloatNumber">0.000001</span><span class="Other">)</span><span class="Other">)</span><span class="Other">:</span> <span class="Identifier">T</span></pre></dt>
<dd>

Numerically calculate the second derivative of f(x) at x0 using a step size h.
&nbsp;&nbsp;<a
href="https://github.com/SciNim/numericalnim/tree/master/src/numericalnim/differentiate.nim#L19"
class="link-seesrc" target="_blank">Source</a>
&nbsp;&nbsp;<a href="https://github.com/SciNim/numericalnim/edit/devel/src/numericalnim/differentiate.nim#L19" class="link-seesrc" target="_blank" >Edit</a>

</dd>
</div>
<div id="tensorGradient,proc(Tensor[U]),Tensor[U],U,bool_2">
<dt><pre><span class="Keyword">proc</span> <a href="#tensorGradient%2Cproc%28Tensor%5BU%5D%29%2CTensor%5BU%5D%2CU%2Cbool_2"><span class="Identifier">tensorGradient</span></a><span class="Other">[</span><span class="Identifier">U</span><span class="Other">,</span> <span class="Identifier">T</span><span class="Other">]</span><span class="Other">(</span><span class="Identifier">f</span><span class="Other">:</span> <span class="Keyword">proc</span> <span class="Other">(</span><span class="Identifier">x</span><span class="Other">:</span> <span class="Identifier">Tensor</span><span class="Other">[</span><span class="Identifier">U</span><span class="Other">]</span><span class="Other">)</span><span class="Other">:</span> <span class="Identifier">Tensor</span><span class="Other">[</span><span class="Identifier">T</span><span class="Other">]</span><span class="Other">;</span> <span class="Identifier">x0</span><span class="Other">:</span> <span class="Identifier">Tensor</span><span class="Other">[</span><span class="Identifier">U</span><span class="Other">]</span><span class="Other">;</span>
                          <span class="Identifier">h</span><span class="Other">:</span> <span class="Identifier">U</span> <span class="Other">=</span> <span class="Identifier">U</span><span class="Other">(</span><span class="FloatNumber">0.000001</span><span class="Other">)</span><span class="Other">;</span> <span class="Identifier">fastMode</span><span class="Other">:</span> <span class="Identifier">bool</span> <span class="Other">=</span> <span class="Identifier">false</span><span class="Other">)</span><span class="Other">:</span> <span class="Identifier">Tensor</span><span class="Other">[</span><span class="Identifier">T</span><span class="Other">]</span></pre></dt>
<dd>

Calculates the gradient of f(x) w.r.t vector x at x0 using step size h. Every column is the gradient of one component of f. By default it uses central difference for approximating the derivatives. This requires two function evaluations per derivative. When fastMode is true it will instead use the forward difference which only uses 1 function evaluation per derivative but is less accurate.
&nbsp;&nbsp;<a
href="https://github.com/SciNim/numericalnim/tree/master/src/numericalnim/differentiate.nim#L58"
class="link-seesrc" target="_blank">Source</a>
&nbsp;&nbsp;<a href="https://github.com/SciNim/numericalnim/edit/devel/src/numericalnim/differentiate.nim#L58" class="link-seesrc" target="_blank" >Edit</a>

</dd>
</div>
<div id="tensorGradient,proc(Tensor[U]),Tensor[U],U,bool">
<dt><pre><span class="Keyword">proc</span> <a href="#tensorGradient%2Cproc%28Tensor%5BU%5D%29%2CTensor%5BU%5D%2CU%2Cbool"><span class="Identifier">tensorGradient</span></a><span class="Other">[</span><span class="Identifier">U</span><span class="Other">;</span> <span class="Identifier">T</span><span class="Other">:</span> <span class="Keyword">not</span> <span class="Identifier">Tensor</span><span class="Other">]</span><span class="Other">(</span><span class="Identifier">f</span><span class="Other">:</span> <span class="Keyword">proc</span> <span class="Other">(</span><span class="Identifier">x</span><span class="Other">:</span> <span class="Identifier">Tensor</span><span class="Other">[</span><span class="Identifier">U</span><span class="Other">]</span><span class="Other">)</span><span class="Other">:</span> <span class="Identifier">T</span><span class="Other">;</span> <span class="Identifier">x0</span><span class="Other">:</span> <span class="Identifier">Tensor</span><span class="Other">[</span><span class="Identifier">U</span><span class="Other">]</span><span class="Other">;</span>
                                      <span class="Identifier">h</span><span class="Other">:</span> <span class="Identifier">U</span> <span class="Other">=</span> <span class="Identifier">U</span><span class="Other">(</span><span class="FloatNumber">0.000001</span><span class="Other">)</span><span class="Other">;</span> <span class="Identifier">fastMode</span><span class="Other">:</span> <span class="Identifier">bool</span> <span class="Other">=</span> <span class="Identifier">false</span><span class="Other">)</span><span class="Other">:</span> <span class="Identifier">Tensor</span><span class="Other">[</span>
    <span class="Identifier">T</span><span class="Other">]</span></pre></dt>
<dd>

Calculates the gradient of f(x) w.r.t vector x at x0 using step size h. By default it uses central difference for approximating the derivatives. This requires two function evaluations per derivative. When fastMode is true it will instead use the forward difference which only uses 1 function evaluation per derivative but is less accurate.
&nbsp;&nbsp;<a
href="https://github.com/SciNim/numericalnim/tree/master/src/numericalnim/differentiate.nim#L32"
class="link-seesrc" target="_blank">Source</a>
&nbsp;&nbsp;<a href="https://github.com/SciNim/numericalnim/edit/devel/src/numericalnim/differentiate.nim#L32" class="link-seesrc" target="_blank" >Edit</a>

</dd>
</div>
<div id="tensorHessian,proc(Tensor[U]),Tensor[U],U">
<dt><pre><span class="Keyword">proc</span> <a href="#tensorHessian%2Cproc%28Tensor%5BU%5D%29%2CTensor%5BU%5D%2CU"><span class="Identifier">tensorHessian</span></a><span class="Other">[</span><span class="Identifier">U</span><span class="Other">;</span> <span class="Identifier">T</span><span class="Other">:</span> <span class="Keyword">not</span> <span class="Identifier">Tensor</span><span class="Other">]</span><span class="Other">(</span><span class="Identifier">f</span><span class="Other">:</span> <span class="Keyword">proc</span> <span class="Other">(</span><span class="Identifier">x</span><span class="Other">:</span> <span class="Identifier">Tensor</span><span class="Other">[</span><span class="Identifier">U</span><span class="Other">]</span><span class="Other">)</span><span class="Other">:</span> <span class="Identifier">T</span><span class="Other">;</span> <span class="Identifier">x0</span><span class="Other">:</span> <span class="Identifier">Tensor</span><span class="Other">[</span><span class="Identifier">U</span><span class="Other">]</span><span class="Other">;</span>
                                     <span class="Identifier">h</span><span class="Other">:</span> <span class="Identifier">U</span> <span class="Other">=</span> <span class="Identifier">U</span><span class="Other">(</span><span class="FloatNumber">0.000001</span><span class="Other">)</span><span class="Other">)</span><span class="Other">:</span> <span class="Identifier">Tensor</span><span class="Other">[</span><span class="Identifier">T</span><span class="Other">]</span></pre></dt>
<dd>


&nbsp;&nbsp;<a
href="https://github.com/SciNim/numericalnim/tree/master/src/numericalnim/differentiate.nim#L127"
class="link-seesrc" target="_blank">Source</a>
&nbsp;&nbsp;<a href="https://github.com/SciNim/numericalnim/edit/devel/src/numericalnim/differentiate.nim#L127" class="link-seesrc" target="_blank" >Edit</a>

</dd>
</div>
<div id="tensorJacobian,proc(Tensor[U]),Tensor[U],U,bool">
<dt><pre><span class="Keyword">proc</span> <a href="#tensorJacobian%2Cproc%28Tensor%5BU%5D%29%2CTensor%5BU%5D%2CU%2Cbool"><span class="Identifier">tensorJacobian</span></a><span class="Other">[</span><span class="Identifier">U</span><span class="Other">,</span> <span class="Identifier">T</span><span class="Other">]</span><span class="Other">(</span><span class="Identifier">f</span><span class="Other">:</span> <span class="Keyword">proc</span> <span class="Other">(</span><span class="Identifier">x</span><span class="Other">:</span> <span class="Identifier">Tensor</span><span class="Other">[</span><span class="Identifier">U</span><span class="Other">]</span><span class="Other">)</span><span class="Other">:</span> <span class="Identifier">Tensor</span><span class="Other">[</span><span class="Identifier">T</span><span class="Other">]</span><span class="Other">;</span> <span class="Identifier">x0</span><span class="Other">:</span> <span class="Identifier">Tensor</span><span class="Other">[</span><span class="Identifier">U</span><span class="Other">]</span><span class="Other">;</span>
                          <span class="Identifier">h</span><span class="Other">:</span> <span class="Identifier">U</span> <span class="Other">=</span> <span class="Identifier">U</span><span class="Other">(</span><span class="FloatNumber">0.000001</span><span class="Other">)</span><span class="Other">;</span> <span class="Identifier">fastMode</span><span class="Other">:</span> <span class="Identifier">bool</span> <span class="Other">=</span> <span class="Identifier">false</span><span class="Other">)</span><span class="Other">:</span> <span class="Identifier">Tensor</span><span class="Other">[</span><span class="Identifier">T</span><span class="Other">]</span></pre></dt>
<dd>

Calculates the jacobian of f(x) w.r.t vector x at x0 using step size h. Every row is the gradient of one component of f. By default it uses central difference for approximating the derivatives. This requires two function evaluations per derivative. When fastMode is true it will instead use the forward difference which only uses 1 function evaluation per derivative but is less accurate.
&nbsp;&nbsp;<a
href="https://github.com/SciNim/numericalnim/tree/master/src/numericalnim/differentiate.nim#L87"
class="link-seesrc" target="_blank">Source</a>
&nbsp;&nbsp;<a href="https://github.com/SciNim/numericalnim/edit/devel/src/numericalnim/differentiate.nim#L87" class="link-seesrc" target="_blank" >Edit</a>

</dd>
</div>

</dl></div>

  </div>
</div>

    <div class="row">
      <div class="twelve-columns footer">
        <span class="nim-sprite"></span>
        <br/>
        <small style="color: var(--hint);">Made with Nim. Generated: 2023-01-05 22:00:43 UTC</small>
      </div>
    </div>
  </div>
</div>

</body>
</html>
