<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<!--  This file is generated by Nim. -->
<html xmlns="https://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>src/numericalnim/differentiate</title>

<!-- Favicon -->
<link rel="shortcut icon" href="data:image/x-icon;base64,AAABAAEAEBAAAAEAIABoBAAAFgAAACgAAAAQAAAAIAAAAAEAIAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD///8A////AP///wD///8A////AP///wD///8A////AP///wD///8A////AP///wD///8A////AP///wD///8A////AP///wD///8A////AP///wD///8A////AAAAAAUAAAAF////AP///wD///8A////AP///wD///8A////AP///wD///8A////AAAAAAIAAABbAAAAlQAAAKIAAACbAAAAmwAAAKIAAACVAAAAWwAAAAL///8A////AP///wD///8A////AAAAABQAAADAAAAAYwAAAA3///8A////AP///wD///8AAAAADQAAAGMAAADAAAAAFP///wD///8A////AP///wAAAACdAAAAOv///wD///8A////AP///wD///8A////AP///wD///8AAAAAOgAAAJ3///8A////AP///wAAAAAnAAAAcP///wAAAAAoAAAASv///wD///8A////AP///wAAAABKAAAAKP///wAAAABwAAAAJ////wD///8AAAAAgQAAABwAAACIAAAAkAAAAJMAAACtAAAAFQAAABUAAACtAAAAkwAAAJAAAACIAAAAHAAAAIH///8A////AAAAAKQAAACrAAAAaP///wD///8AAAAARQAAANIAAADSAAAARf///wD///8AAAAAaAAAAKsAAACk////AAAAADMAAACcAAAAnQAAABj///8A////AP///wAAAAAYAAAAGP///wD///8A////AAAAABgAAACdAAAAnAAAADMAAAB1AAAAwwAAAP8AAADpAAAAsQAAAE4AAAAb////AP///wAAAAAbAAAATgAAALEAAADpAAAA/wAAAMMAAAB1AAAAtwAAAOkAAAD/AAAA/wAAAP8AAADvAAAA3gAAAN4AAADeAAAA3gAAAO8AAAD/AAAA/wAAAP8AAADpAAAAtwAAAGUAAAA/AAAA3wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAADfAAAAPwAAAGX///8A////AAAAAEgAAADtAAAAvwAAAL0AAADGAAAA7wAAAO8AAADGAAAAvQAAAL8AAADtAAAASP///wD///8A////AP///wD///8AAAAAO////wD///8A////AAAAAIcAAACH////AP///wD///8AAAAAO////wD///8A////AP///wD///8A////AP///wD///8A////AP///wD///8A////AP///wD///8A////AP///wD///8A////AP///wD///8A////AP///wD///8A////AP///wD///8A////AP///wD///8A////AP///wD///8A////AP///wD///8A//8AAP//AAD4HwAA7/cAAN/7AAD//wAAoYUAAJ55AACf+QAAh+EAAAAAAADAAwAA4AcAAP5/AAD//wAA//8AAA=="/>
<link rel="icon" type="image/png" sizes="32x32" href="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAABmJLR0QA/wD/AP+gvaeTAAAACXBIWXMAAA3XAAAN1wFCKJt4AAAAB3RJTUUH4QQQEwksSS9ZWwAAAk1JREFUWMPtll2ITVEUx39nn/O7Y5qR8f05wtCUUr6ZIS++8pEnkZInPImneaCQ5METNdOkeFBKUhMPRIkHKfEuUZSUlGlKPN2TrgfncpvmnntnmlEyq1Z7t89/rf9a6+y99oZxGZf/XeIq61EdtgKXgdXA0xrYAvBjOIF1AI9zvjcC74BSpndrJPkBWDScTF8Aa4E3wDlgHbASaANmVqlcCnwHvgDvgVfAJ+AikAAvgfVZwLnSVZHZaOuKoQi3ZOMi4NkYkpe1p4J7A8BpYAD49hfIy/oqG0+hLomiKP2L5L+1ubn5115S+3OAn4EnwBlgMzCjyt6ZAnQCJ4A7wOs88iRJHvw50HoujuPBoCKwHWiosy8MdfZnAdcHk8dxXFJ3VQbQlCTJvRBCGdRbD4M6uc5glpY3eAihpN5S5w12diSEcCCEcKUO4ljdr15T76ur1FDDLIQQ3qv71EdDOe3Kxj3leRXyk+pxdWnFWod6Wt2bY3de3aSuUHcPBVimHs7mK9WrmeOF6lR1o9qnzskh2ar2qm1qizpfXaPeVGdlmGN5pb09qMxz1Xb1kLqgzn1RyH7JUXW52lr5e/Kqi9qpto7V1atuUzfnARrV7jEib1T76gG2qxdGmXyiekkt1GswPTtek0aBfJp6YySGBfWg2tPQ0FAYgf1stUfdmdcjarbYJEniKIq6gY/Aw+zWHAC+p2labGpqiorFYgGYCEzN7oQdQClN07O1/EfDyGgC0ALMBdYAi4FyK+4H3gLPsxfR1zRNi+NP7nH5J+QntnXe5B5mpfQAAAAASUVORK5CYII=">

<!-- CSS -->
<link rel="stylesheet" type="text/css" href="../nimdoc.out.css?v=2.0.8">

<!-- JS -->
<script type="text/javascript" src="../dochack.js?v=2.0.8"></script>
</head>
<body>
  <div class="document" id="documentId">
    <div class="container">
      <h1 class="title">src/numericalnim/differentiate</h1>
      <div class="row">
  <div class="three columns">
    <div class="theme-select-wrapper">
      <label for="theme-select">Theme:&nbsp;</label>
      <select id="theme-select" onchange="setTheme(this.value)">
        <option value="auto">ðŸŒ— Match OS</option>
        <option value="dark">ðŸŒ‘ Dark</option>
        <option value="light">ðŸŒ• Light</option>
      </select>
    </div>
    <div id="global-links">
      <ul class="simple">
        <li><a id="indexLink" href="../theindex.html">Index</a></li>
      </ul>
    </div>
    <div id="searchInputDiv">
      Search: <input type="search" id="searchInput" onkeyup="search()"/>
    </div>
    <div>
      Group by:
      <select onchange="groupBy(this.value)">
        <option value="section">Section</option>
        <option value="type">Type</option>
      </select>
    </div>
    <ul class="simple simple-toc" id="toc-list">
  <li><a class="reference" id="differentiation_toc" href="#differentiation">Differentiation</a></li>
<li>
  <details open>
    <summary><a class="reference reference-toplevel" href="#12" id="62">Procs</a></summary>
    <ul class="simple simple-toc-section">
      <ul class="simple nested-toc-section">checkGradient
  <li><a class="reference" href="#checkGradient%2Cproc%28Tensor%5BU%5D%29%2Cproc%28Tensor%5BU%5D%29%2CTensor%5BU%5D%2CT" title="checkGradient[U; T: not Tensor](f: proc (x: Tensor[U]): T;
                                fGrad: proc (x: Tensor[U]): Tensor[T];
                                x0: Tensor[U]; tol: T): bool">checkGradient[U; T: not Tensor](f: proc (x: Tensor[U]): T;
                                fGrad: proc (x: Tensor[U]): Tensor[T];
                                x0: Tensor[U]; tol: T): bool</a></li>
<li><a class="reference" href="#checkGradient%2Cproc%28Tensor%5BU%5D%29%2Cproc%28Tensor%5BU%5D%29%2CTensor%5BU%5D%2CT_2" title="checkGradient[U; T](f: proc (x: Tensor[U]): Tensor[T];
                    fGrad: proc (x: Tensor[U]): Tensor[T]; x0: Tensor[U]; tol: T): bool">checkGradient[U; T](f: proc (x: Tensor[U]): Tensor[T];
                    fGrad: proc (x: Tensor[U]): Tensor[T]; x0: Tensor[U]; tol: T): bool</a></li>

</ul>
<ul class="simple nested-toc-section">diff1dBackward
  <li><a class="reference" href="#diff1dBackward%2Cproc%28U%29%2CU%2CU" title="diff1dBackward[U, T](f: proc (x: U): T; x0: U; h: U = U(0.000001)): T">diff1dBackward[U, T](f: proc (x: U): T; x0: U; h: U = U(0.000001)): T</a></li>

</ul>
<ul class="simple nested-toc-section">diff1dCentral
  <li><a class="reference" href="#diff1dCentral%2Cproc%28U%29%2CU%2CU" title="diff1dCentral[U, T](f: proc (x: U): T; x0: U; h: U = U(0.000001)): T">diff1dCentral[U, T](f: proc (x: U): T; x0: U; h: U = U(0.000001)): T</a></li>

</ul>
<ul class="simple nested-toc-section">diff1dForward
  <li><a class="reference" href="#diff1dForward%2Cproc%28U%29%2CU%2CU" title="diff1dForward[U, T](f: proc (x: U): T; x0: U; h: U = U(0.000001)): T">diff1dForward[U, T](f: proc (x: U): T; x0: U; h: U = U(0.000001)): T</a></li>

</ul>
<ul class="simple nested-toc-section">mixedDerivative
  <li><a class="reference" href="#mixedDerivative%2Cproc%28Tensor%5BU%5D%29%2CTensor%5BU%5D%2C%2CU" title="mixedDerivative[U, T](f: proc (x: Tensor[U]): T; x0: var Tensor[U];
                      indices: (int, int); h: U = U(0.000001)): T">mixedDerivative[U, T](f: proc (x: Tensor[U]): T; x0: var Tensor[U];
                      indices: (int, int); h: U = U(0.000001)): T</a></li>

</ul>
<ul class="simple nested-toc-section">secondDiff1dBackward
  <li><a class="reference" href="#secondDiff1dBackward%2Cproc%28U%29%2CU%2CU" title="secondDiff1dBackward[U, T](f: proc (x: U): T; x0: U; h: U = U(0.000001)): T">secondDiff1dBackward[U, T](f: proc (x: U): T; x0: U; h: U = U(0.000001)): T</a></li>

</ul>
<ul class="simple nested-toc-section">secondDiff1dCentral
  <li><a class="reference" href="#secondDiff1dCentral%2Cproc%28U%29%2CU%2CU" title="secondDiff1dCentral[U, T](f: proc (x: U): T; x0: U; h: U = U(0.000001)): T">secondDiff1dCentral[U, T](f: proc (x: U): T; x0: U; h: U = U(0.000001)): T</a></li>

</ul>
<ul class="simple nested-toc-section">secondDiff1dForward
  <li><a class="reference" href="#secondDiff1dForward%2Cproc%28U%29%2CU%2CU" title="secondDiff1dForward[U, T](f: proc (x: U): T; x0: U; h: U = U(0.000001)): T">secondDiff1dForward[U, T](f: proc (x: U): T; x0: U; h: U = U(0.000001)): T</a></li>

</ul>
<ul class="simple nested-toc-section">tensorGradient
  <li><a class="reference" href="#tensorGradient%2Cproc%28Tensor%5BU%5D%29%2CTensor%5BU%5D%2CU%2Cbool_2" title="tensorGradient[U, T](f: proc (x: Tensor[U]): Tensor[T]; x0: Tensor[U];
                     h: U = U(0.000001); fastMode: bool = false): Tensor[T]">tensorGradient[U, T](f: proc (x: Tensor[U]): Tensor[T]; x0: Tensor[U];
                     h: U = U(0.000001); fastMode: bool = false): Tensor[T]</a></li>
<li><a class="reference" href="#tensorGradient%2Cproc%28Tensor%5BU%5D%29%2CTensor%5BU%5D%2CU%2Cbool" title="tensorGradient[U; T: not Tensor](f: proc (x: Tensor[U]): T; x0: Tensor[U];
                                 h: U = U(0.000001); fastMode: bool = false): Tensor[
    T]">tensorGradient[U; T: not Tensor](f: proc (x: Tensor[U]): T; x0: Tensor[U];
                                 h: U = U(0.000001); fastMode: bool = false): Tensor[
    T]</a></li>

</ul>
<ul class="simple nested-toc-section">tensorHessian
  <li><a class="reference" href="#tensorHessian%2Cproc%28Tensor%5BU%5D%29%2CTensor%5BU%5D%2CU" title="tensorHessian[U; T: not Tensor](f: proc (x: Tensor[U]): T; x0: Tensor[U];
                                h: U = U(0.000001)): Tensor[T]">tensorHessian[U; T: not Tensor](f: proc (x: Tensor[U]): T; x0: Tensor[U];
                                h: U = U(0.000001)): Tensor[T]</a></li>

</ul>
<ul class="simple nested-toc-section">tensorJacobian
  <li><a class="reference" href="#tensorJacobian%2Cproc%28Tensor%5BU%5D%29%2CTensor%5BU%5D%2CU%2Cbool" title="tensorJacobian[U, T](f: proc (x: Tensor[U]): Tensor[T]; x0: Tensor[U];
                     h: U = U(0.000001); fastMode: bool = false): Tensor[T]">tensorJacobian[U, T](f: proc (x: Tensor[U]): Tensor[T]; x0: Tensor[U];
                     h: U = U(0.000001); fastMode: bool = false): Tensor[T]</a></li>

</ul>

    </ul>
  </details>
</li>

</ul>

  </div>
  <div class="nine columns" id="content">
    <a href="https://github.com/SciNim/numericalnim/tree/master/src/numericalnim/differentiate.nim#L1" class="link-seesrc" target="_blank">Source</a>&nbsp;&nbsp;
<a href="https://github.com/SciNim/numericalnim/edit/devel/src/numericalnim/differentiate.nim#L1" class="link-seesrc" target="_blank" >Edit</a>&nbsp;&nbsp;

    <div id="tocRoot"></div>
    
    <p class="module-desc">
<h1><a class="toc-backref" id="differentiation" href="#differentiation">Differentiation</a></h1><p>This module implements various finite difference methods for approximating derivatives.</p>
<p>For multi-valued functions, it is expected that both the inputs and outputs are 1D <tt class="docutils literal"><span class="pre"><span class="Identifier">Tensor</span></span></tt>s.</p>
<p>Here is an example with a scalar multi-variate function:</p>

<p><strong class="examples_text">Example:</strong></p>
<pre class="listing"><span class="Keyword">import</span> <span class="Identifier">src</span><span class="Operator">/</span><span class="Identifier">numericalnim</span><span class="Operator">/</span><span class="Identifier">differentiate</span>
<span class="Keyword">import</span> <span class="Identifier">arraymancer</span><span class="Punctuation">,</span> <span class="Identifier">std</span><span class="Operator">/</span><span class="Identifier">math</span>
<span class="Comment"># f(x, y) = sin(x) + 2*sin(y)</span>
<span class="Keyword">proc</span> <span class="Identifier">f</span><span class="Punctuation">(</span><span class="Identifier">x</span><span class="Punctuation">:</span> <span class="Identifier">Tensor</span><span class="Punctuation">[</span><span class="Identifier">float</span><span class="Punctuation">]</span><span class="Punctuation">)</span><span class="Punctuation">:</span> <span class="Identifier">float</span> <span class="Operator">=</span>
  <span class="Identifier">sin</span><span class="Punctuation">(</span><span class="Identifier">x</span><span class="Punctuation">[</span><span class="DecNumber">0</span><span class="Punctuation">]</span><span class="Punctuation">)</span> <span class="Operator">+</span> <span class="DecNumber">2</span><span class="Operator">*</span><span class="Identifier">sin</span><span class="Punctuation">(</span><span class="Identifier">x</span><span class="Punctuation">[</span><span class="DecNumber">1</span><span class="Punctuation">]</span><span class="Punctuation">)</span>

<span class="Keyword">let</span> <span class="Identifier">x</span> <span class="Operator">=</span> <span class="Punctuation">[</span><span class="FloatNumber">1.0</span><span class="Punctuation">,</span> <span class="FloatNumber">1.0</span><span class="Punctuation">]</span><span class="Operator">.</span><span class="Identifier">toTensor</span>
<span class="Keyword">let</span> <span class="Identifier">gradient</span> <span class="Operator">=</span> <span class="Identifier">tensorGradient</span><span class="Punctuation">(</span><span class="Identifier">f</span><span class="Punctuation">,</span> <span class="Identifier">x</span><span class="Punctuation">)</span></pre>Here's an example with a multi-valued function:
<p><strong class="examples_text">Example:</strong></p>
<pre class="listing"><span class="Keyword">import</span> <span class="Identifier">src</span><span class="Operator">/</span><span class="Identifier">numericalnim</span><span class="Operator">/</span><span class="Identifier">differentiate</span>
<span class="Keyword">import</span> <span class="Identifier">arraymancer</span><span class="Punctuation">,</span> <span class="Identifier">std</span><span class="Operator">/</span><span class="Identifier">math</span>
<span class="Comment"># f(x, y) = [sin(x*y), cos(x*y)]</span>
<span class="Keyword">proc</span> <span class="Identifier">f</span><span class="Punctuation">(</span><span class="Identifier">x</span><span class="Punctuation">:</span> <span class="Identifier">Tensor</span><span class="Punctuation">[</span><span class="Identifier">float</span><span class="Punctuation">]</span><span class="Punctuation">)</span><span class="Punctuation">:</span> <span class="Identifier">Tensor</span><span class="Punctuation">[</span><span class="Identifier">float</span><span class="Punctuation">]</span> <span class="Operator">=</span>
  <span class="Identifier">result</span> <span class="Operator">=</span> <span class="Identifier">newTensor</span><span class="Punctuation">[</span><span class="Identifier">float</span><span class="Punctuation">]</span><span class="Punctuation">(</span><span class="DecNumber">2</span><span class="Punctuation">)</span>
  <span class="Keyword">let</span> <span class="Identifier">arg</span> <span class="Operator">=</span> <span class="Identifier">x</span><span class="Punctuation">[</span><span class="DecNumber">0</span><span class="Punctuation">]</span> <span class="Operator">*</span> <span class="Identifier">x</span><span class="Punctuation">[</span><span class="DecNumber">1</span><span class="Punctuation">]</span>
  <span class="Identifier">result</span><span class="Punctuation">[</span><span class="DecNumber">0</span><span class="Punctuation">]</span> <span class="Operator">=</span> <span class="Identifier">sin</span><span class="Punctuation">(</span><span class="Identifier">arg</span><span class="Punctuation">)</span>
  <span class="Identifier">result</span><span class="Punctuation">[</span><span class="DecNumber">1</span><span class="Punctuation">]</span> <span class="Operator">=</span> <span class="Identifier">cos</span><span class="Punctuation">(</span><span class="Identifier">arg</span><span class="Punctuation">)</span>

<span class="Keyword">let</span> <span class="Identifier">x</span> <span class="Operator">=</span> <span class="Punctuation">[</span><span class="FloatNumber">1.0</span><span class="Punctuation">,</span> <span class="FloatNumber">1.0</span><span class="Punctuation">]</span><span class="Operator">.</span><span class="Identifier">toTensor</span>
<span class="Keyword">let</span> <span class="Identifier">jacobian</span> <span class="Operator">=</span> <span class="Identifier">tensorJacobian</span><span class="Punctuation">(</span><span class="Identifier">f</span><span class="Punctuation">,</span> <span class="Identifier">x</span><span class="Punctuation">)</span></pre></p>
    <div class="section" id="12">
  <h1><a class="toc-backref" href="#12">Procs</a></h1>
  <dl class="item">
    <div id="checkGradient-procs-all">
  <div id="checkGradient,proc(Tensor[U]),proc(Tensor[U]),Tensor[U],T">
  <dt><pre><span class="Keyword">proc</span> <a href="#checkGradient%2Cproc%28Tensor%5BU%5D%29%2Cproc%28Tensor%5BU%5D%29%2CTensor%5BU%5D%2CT"><span class="Identifier">checkGradient</span></a><span class="Other">[</span><span class="Identifier">U</span><span class="Other">;</span> <span class="Identifier">T</span><span class="Other">:</span> <span class="Keyword">not</span> <span class="Identifier">Tensor</span><span class="Other">]</span><span class="Other">(</span><span class="Identifier">f</span><span class="Other">:</span> <span class="Keyword">proc</span> <span class="Other">(</span><span class="Identifier">x</span><span class="Other">:</span> <span class="Identifier">Tensor</span><span class="Other">[</span><span class="Identifier">U</span><span class="Other">]</span><span class="Other">)</span><span class="Other">:</span> <span class="Identifier">T</span><span class="Other">;</span>
                                     <span class="Identifier">fGrad</span><span class="Other">:</span> <span class="Keyword">proc</span> <span class="Other">(</span><span class="Identifier">x</span><span class="Other">:</span> <span class="Identifier">Tensor</span><span class="Other">[</span><span class="Identifier">U</span><span class="Other">]</span><span class="Other">)</span><span class="Other">:</span> <span class="Identifier">Tensor</span><span class="Other">[</span><span class="Identifier">T</span><span class="Other">]</span><span class="Other">;</span>
                                     <span class="Identifier">x0</span><span class="Other">:</span> <span class="Identifier">Tensor</span><span class="Other">[</span><span class="Identifier">U</span><span class="Other">]</span><span class="Other">;</span> <span class="Identifier">tol</span><span class="Other">:</span> <span class="Identifier">T</span><span class="Other">)</span><span class="Other">:</span> <span class="Identifier">bool</span></pre></dt>
  <dd>
    
    Checks if the provided gradient function <tt class="docutils literal"><span class="pre"><span class="Identifier">fGrad</span></span></tt> gives the same values as numeric gradient.
    <a href="https://github.com/SciNim/numericalnim/tree/master/src/numericalnim/differentiate.nim#L192" class="link-seesrc" target="_blank">Source</a>&nbsp;&nbsp;
<a href="https://github.com/SciNim/numericalnim/edit/devel/src/numericalnim/differentiate.nim#L192" class="link-seesrc" target="_blank" >Edit</a>&nbsp;&nbsp;

  </dd>
</div>
<div id="checkGradient,proc(Tensor[U]),proc(Tensor[U]),Tensor[U],T_2">
  <dt><pre><span class="Keyword">proc</span> <a href="#checkGradient%2Cproc%28Tensor%5BU%5D%29%2Cproc%28Tensor%5BU%5D%29%2CTensor%5BU%5D%2CT_2"><span class="Identifier">checkGradient</span></a><span class="Other">[</span><span class="Identifier">U</span><span class="Other">;</span> <span class="Identifier">T</span><span class="Other">]</span><span class="Other">(</span><span class="Identifier">f</span><span class="Other">:</span> <span class="Keyword">proc</span> <span class="Other">(</span><span class="Identifier">x</span><span class="Other">:</span> <span class="Identifier">Tensor</span><span class="Other">[</span><span class="Identifier">U</span><span class="Other">]</span><span class="Other">)</span><span class="Other">:</span> <span class="Identifier">Tensor</span><span class="Other">[</span><span class="Identifier">T</span><span class="Other">]</span><span class="Other">;</span>
                         <span class="Identifier">fGrad</span><span class="Other">:</span> <span class="Keyword">proc</span> <span class="Other">(</span><span class="Identifier">x</span><span class="Other">:</span> <span class="Identifier">Tensor</span><span class="Other">[</span><span class="Identifier">U</span><span class="Other">]</span><span class="Other">)</span><span class="Other">:</span> <span class="Identifier">Tensor</span><span class="Other">[</span><span class="Identifier">T</span><span class="Other">]</span><span class="Other">;</span> <span class="Identifier">x0</span><span class="Other">:</span> <span class="Identifier">Tensor</span><span class="Other">[</span><span class="Identifier">U</span><span class="Other">]</span><span class="Other">;</span>
                         <span class="Identifier">tol</span><span class="Other">:</span> <span class="Identifier">T</span><span class="Other">)</span><span class="Other">:</span> <span class="Identifier">bool</span></pre></dt>
  <dd>
    
    Checks if the provided gradient function <tt class="docutils literal"><span class="pre"><span class="Identifier">fGrad</span></span></tt> gives the same values as numeric gradient.
    <a href="https://github.com/SciNim/numericalnim/tree/master/src/numericalnim/differentiate.nim#L202" class="link-seesrc" target="_blank">Source</a>&nbsp;&nbsp;
<a href="https://github.com/SciNim/numericalnim/edit/devel/src/numericalnim/differentiate.nim#L202" class="link-seesrc" target="_blank" >Edit</a>&nbsp;&nbsp;

  </dd>
</div>

</div>
<div id="diff1dBackward-procs-all">
  <div id="diff1dBackward,proc(U),U,U">
  <dt><pre><span class="Keyword">proc</span> <a href="#diff1dBackward%2Cproc%28U%29%2CU%2CU"><span class="Identifier">diff1dBackward</span></a><span class="Other">[</span><span class="Identifier">U</span><span class="Other">,</span> <span class="Identifier">T</span><span class="Other">]</span><span class="Other">(</span><span class="Identifier">f</span><span class="Other">:</span> <span class="Keyword">proc</span> <span class="Other">(</span><span class="Identifier">x</span><span class="Other">:</span> <span class="Identifier">U</span><span class="Other">)</span><span class="Other">:</span> <span class="Identifier">T</span><span class="Other">;</span> <span class="Identifier">x0</span><span class="Other">:</span> <span class="Identifier">U</span><span class="Other">;</span> <span class="Identifier">h</span><span class="Other">:</span> <span class="Identifier">U</span> <span class="Other">=</span> <span class="Identifier">U</span><span class="Other">(</span><span class="FloatNumber">0.000001</span><span class="Other">)</span><span class="Other">)</span><span class="Other">:</span> <span class="Identifier">T</span></pre></dt>
  <dd>
    
    Numerically calculate the derivative of f(x) at x0 using a step size h. Uses backward difference which has accuracy O(h)
    <a href="https://github.com/SciNim/numericalnim/tree/master/src/numericalnim/differentiate.nim#L41" class="link-seesrc" target="_blank">Source</a>&nbsp;&nbsp;
<a href="https://github.com/SciNim/numericalnim/edit/devel/src/numericalnim/differentiate.nim#L41" class="link-seesrc" target="_blank" >Edit</a>&nbsp;&nbsp;

  </dd>
</div>

</div>
<div id="diff1dCentral-procs-all">
  <div id="diff1dCentral,proc(U),U,U">
  <dt><pre><span class="Keyword">proc</span> <a href="#diff1dCentral%2Cproc%28U%29%2CU%2CU"><span class="Identifier">diff1dCentral</span></a><span class="Other">[</span><span class="Identifier">U</span><span class="Other">,</span> <span class="Identifier">T</span><span class="Other">]</span><span class="Other">(</span><span class="Identifier">f</span><span class="Other">:</span> <span class="Keyword">proc</span> <span class="Other">(</span><span class="Identifier">x</span><span class="Other">:</span> <span class="Identifier">U</span><span class="Other">)</span><span class="Other">:</span> <span class="Identifier">T</span><span class="Other">;</span> <span class="Identifier">x0</span><span class="Other">:</span> <span class="Identifier">U</span><span class="Other">;</span> <span class="Identifier">h</span><span class="Other">:</span> <span class="Identifier">U</span> <span class="Other">=</span> <span class="Identifier">U</span><span class="Other">(</span><span class="FloatNumber">0.000001</span><span class="Other">)</span><span class="Other">)</span><span class="Other">:</span> <span class="Identifier">T</span></pre></dt>
  <dd>
    
    Numerically calculate the derivative of f(x) at x0 using a step size h. Uses central difference which has accuracy O(h^2)
    <a href="https://github.com/SciNim/numericalnim/tree/master/src/numericalnim/differentiate.nim#L46" class="link-seesrc" target="_blank">Source</a>&nbsp;&nbsp;
<a href="https://github.com/SciNim/numericalnim/edit/devel/src/numericalnim/differentiate.nim#L46" class="link-seesrc" target="_blank" >Edit</a>&nbsp;&nbsp;

  </dd>
</div>

</div>
<div id="diff1dForward-procs-all">
  <div id="diff1dForward,proc(U),U,U">
  <dt><pre><span class="Keyword">proc</span> <a href="#diff1dForward%2Cproc%28U%29%2CU%2CU"><span class="Identifier">diff1dForward</span></a><span class="Other">[</span><span class="Identifier">U</span><span class="Other">,</span> <span class="Identifier">T</span><span class="Other">]</span><span class="Other">(</span><span class="Identifier">f</span><span class="Other">:</span> <span class="Keyword">proc</span> <span class="Other">(</span><span class="Identifier">x</span><span class="Other">:</span> <span class="Identifier">U</span><span class="Other">)</span><span class="Other">:</span> <span class="Identifier">T</span><span class="Other">;</span> <span class="Identifier">x0</span><span class="Other">:</span> <span class="Identifier">U</span><span class="Other">;</span> <span class="Identifier">h</span><span class="Other">:</span> <span class="Identifier">U</span> <span class="Other">=</span> <span class="Identifier">U</span><span class="Other">(</span><span class="FloatNumber">0.000001</span><span class="Other">)</span><span class="Other">)</span><span class="Other">:</span> <span class="Identifier">T</span></pre></dt>
  <dd>
    
    Numerically calculate the derivative of f(x) at x0 using a step size h. Uses forward difference which has accuracy O(h)
    <a href="https://github.com/SciNim/numericalnim/tree/master/src/numericalnim/differentiate.nim#L36" class="link-seesrc" target="_blank">Source</a>&nbsp;&nbsp;
<a href="https://github.com/SciNim/numericalnim/edit/devel/src/numericalnim/differentiate.nim#L36" class="link-seesrc" target="_blank" >Edit</a>&nbsp;&nbsp;

  </dd>
</div>

</div>
<div id="mixedDerivative-procs-all">
  <div id="mixedDerivative,proc(Tensor[U]),Tensor[U],,U">
  <dt><pre><span class="Keyword">proc</span> <a href="#mixedDerivative%2Cproc%28Tensor%5BU%5D%29%2CTensor%5BU%5D%2C%2CU"><span class="Identifier">mixedDerivative</span></a><span class="Other">[</span><span class="Identifier">U</span><span class="Other">,</span> <span class="Identifier">T</span><span class="Other">]</span><span class="Other">(</span><span class="Identifier">f</span><span class="Other">:</span> <span class="Keyword">proc</span> <span class="Other">(</span><span class="Identifier">x</span><span class="Other">:</span> <span class="Identifier">Tensor</span><span class="Other">[</span><span class="Identifier">U</span><span class="Other">]</span><span class="Other">)</span><span class="Other">:</span> <span class="Identifier">T</span><span class="Other">;</span> <span class="Identifier">x0</span><span class="Other">:</span> <span class="Keyword">var</span> <span class="Identifier">Tensor</span><span class="Other">[</span><span class="Identifier">U</span><span class="Other">]</span><span class="Other">;</span>
                           <span class="Identifier">indices</span><span class="Other">:</span> <span class="Other">(</span><span class="Identifier">int</span><span class="Other">,</span> <span class="Identifier">int</span><span class="Other">)</span><span class="Other">;</span> <span class="Identifier">h</span><span class="Other">:</span> <span class="Identifier">U</span> <span class="Other">=</span> <span class="Identifier">U</span><span class="Other">(</span><span class="FloatNumber">0.000001</span><span class="Other">)</span><span class="Other">)</span><span class="Other">:</span> <span class="Identifier">T</span></pre></dt>
  <dd>
    
    Used internally in <tt class="docutils literal"><span class="pre"><span class="Identifier">tensorHessian</span></span></tt>. Calculates the mixed derivative dÂ²f/(dx_i dx_j). Modifies x0. 
    <a href="https://github.com/SciNim/numericalnim/tree/master/src/numericalnim/differentiate.nim#L141" class="link-seesrc" target="_blank">Source</a>&nbsp;&nbsp;
<a href="https://github.com/SciNim/numericalnim/edit/devel/src/numericalnim/differentiate.nim#L141" class="link-seesrc" target="_blank" >Edit</a>&nbsp;&nbsp;

  </dd>
</div>

</div>
<div id="secondDiff1dBackward-procs-all">
  <div id="secondDiff1dBackward,proc(U),U,U">
  <dt><pre><span class="Keyword">proc</span> <a href="#secondDiff1dBackward%2Cproc%28U%29%2CU%2CU"><span class="Identifier">secondDiff1dBackward</span></a><span class="Other">[</span><span class="Identifier">U</span><span class="Other">,</span> <span class="Identifier">T</span><span class="Other">]</span><span class="Other">(</span><span class="Identifier">f</span><span class="Other">:</span> <span class="Keyword">proc</span> <span class="Other">(</span><span class="Identifier">x</span><span class="Other">:</span> <span class="Identifier">U</span><span class="Other">)</span><span class="Other">:</span> <span class="Identifier">T</span><span class="Other">;</span> <span class="Identifier">x0</span><span class="Other">:</span> <span class="Identifier">U</span><span class="Other">;</span> <span class="Identifier">h</span><span class="Other">:</span> <span class="Identifier">U</span> <span class="Other">=</span> <span class="Identifier">U</span><span class="Other">(</span><span class="FloatNumber">0.000001</span><span class="Other">)</span><span class="Other">)</span><span class="Other">:</span> <span class="Identifier">T</span></pre></dt>
  <dd>
    
    Numerically calculate the second derivative of f(x) at x0 using a step size h.
    <a href="https://github.com/SciNim/numericalnim/tree/master/src/numericalnim/differentiate.nim#L55" class="link-seesrc" target="_blank">Source</a>&nbsp;&nbsp;
<a href="https://github.com/SciNim/numericalnim/edit/devel/src/numericalnim/differentiate.nim#L55" class="link-seesrc" target="_blank" >Edit</a>&nbsp;&nbsp;

  </dd>
</div>

</div>
<div id="secondDiff1dCentral-procs-all">
  <div id="secondDiff1dCentral,proc(U),U,U">
  <dt><pre><span class="Keyword">proc</span> <a href="#secondDiff1dCentral%2Cproc%28U%29%2CU%2CU"><span class="Identifier">secondDiff1dCentral</span></a><span class="Other">[</span><span class="Identifier">U</span><span class="Other">,</span> <span class="Identifier">T</span><span class="Other">]</span><span class="Other">(</span><span class="Identifier">f</span><span class="Other">:</span> <span class="Keyword">proc</span> <span class="Other">(</span><span class="Identifier">x</span><span class="Other">:</span> <span class="Identifier">U</span><span class="Other">)</span><span class="Other">:</span> <span class="Identifier">T</span><span class="Other">;</span> <span class="Identifier">x0</span><span class="Other">:</span> <span class="Identifier">U</span><span class="Other">;</span> <span class="Identifier">h</span><span class="Other">:</span> <span class="Identifier">U</span> <span class="Other">=</span> <span class="Identifier">U</span><span class="Other">(</span><span class="FloatNumber">0.000001</span><span class="Other">)</span><span class="Other">)</span><span class="Other">:</span> <span class="Identifier">T</span></pre></dt>
  <dd>
    
    Numerically calculate the second derivative of f(x) at x0 using a step size h. Uses central difference which has accuracy O(h^2)
    <a href="https://github.com/SciNim/numericalnim/tree/master/src/numericalnim/differentiate.nim#L59" class="link-seesrc" target="_blank">Source</a>&nbsp;&nbsp;
<a href="https://github.com/SciNim/numericalnim/edit/devel/src/numericalnim/differentiate.nim#L59" class="link-seesrc" target="_blank" >Edit</a>&nbsp;&nbsp;

  </dd>
</div>

</div>
<div id="secondDiff1dForward-procs-all">
  <div id="secondDiff1dForward,proc(U),U,U">
  <dt><pre><span class="Keyword">proc</span> <a href="#secondDiff1dForward%2Cproc%28U%29%2CU%2CU"><span class="Identifier">secondDiff1dForward</span></a><span class="Other">[</span><span class="Identifier">U</span><span class="Other">,</span> <span class="Identifier">T</span><span class="Other">]</span><span class="Other">(</span><span class="Identifier">f</span><span class="Other">:</span> <span class="Keyword">proc</span> <span class="Other">(</span><span class="Identifier">x</span><span class="Other">:</span> <span class="Identifier">U</span><span class="Other">)</span><span class="Other">:</span> <span class="Identifier">T</span><span class="Other">;</span> <span class="Identifier">x0</span><span class="Other">:</span> <span class="Identifier">U</span><span class="Other">;</span> <span class="Identifier">h</span><span class="Other">:</span> <span class="Identifier">U</span> <span class="Other">=</span> <span class="Identifier">U</span><span class="Other">(</span><span class="FloatNumber">0.000001</span><span class="Other">)</span><span class="Other">)</span><span class="Other">:</span> <span class="Identifier">T</span></pre></dt>
  <dd>
    
    Numerically calculate the second derivative of f(x) at x0 using a step size h.
    <a href="https://github.com/SciNim/numericalnim/tree/master/src/numericalnim/differentiate.nim#L51" class="link-seesrc" target="_blank">Source</a>&nbsp;&nbsp;
<a href="https://github.com/SciNim/numericalnim/edit/devel/src/numericalnim/differentiate.nim#L51" class="link-seesrc" target="_blank" >Edit</a>&nbsp;&nbsp;

  </dd>
</div>

</div>
<div id="tensorGradient-procs-all">
  <div id="tensorGradient,proc(Tensor[U]),Tensor[U],U,bool_2">
  <dt><pre><span class="Keyword">proc</span> <a href="#tensorGradient%2Cproc%28Tensor%5BU%5D%29%2CTensor%5BU%5D%2CU%2Cbool_2"><span class="Identifier">tensorGradient</span></a><span class="Other">[</span><span class="Identifier">U</span><span class="Other">,</span> <span class="Identifier">T</span><span class="Other">]</span><span class="Other">(</span><span class="Identifier">f</span><span class="Other">:</span> <span class="Keyword">proc</span> <span class="Other">(</span><span class="Identifier">x</span><span class="Other">:</span> <span class="Identifier">Tensor</span><span class="Other">[</span><span class="Identifier">U</span><span class="Other">]</span><span class="Other">)</span><span class="Other">:</span> <span class="Identifier">Tensor</span><span class="Other">[</span><span class="Identifier">T</span><span class="Other">]</span><span class="Other">;</span> <span class="Identifier">x0</span><span class="Other">:</span> <span class="Identifier">Tensor</span><span class="Other">[</span><span class="Identifier">U</span><span class="Other">]</span><span class="Other">;</span>
                          <span class="Identifier">h</span><span class="Other">:</span> <span class="Identifier">U</span> <span class="Other">=</span> <span class="Identifier">U</span><span class="Other">(</span><span class="FloatNumber">0.000001</span><span class="Other">)</span><span class="Other">;</span> <span class="Identifier">fastMode</span><span class="Other">:</span> <span class="Identifier">bool</span> <span class="Other">=</span> <span class="Identifier">false</span><span class="Other">)</span><span class="Other">:</span> <span class="Identifier">Tensor</span><span class="Other">[</span><span class="Identifier">T</span><span class="Other">]</span></pre></dt>
  <dd>
    
    <p>Calculates the gradient of multi-valued f(x) w.r.t vector x at x0 using step size h. f(x) is expected to take as input and return a 1D Tensor. Every column is the gradient of one component of f. By default it uses central difference for approximating the derivatives. This requires two function evaluations per derivative. When fastMode is true it will instead use the forward difference which only uses 1 function evaluation per derivative but is less accurate.</p>
<p>Returns:</p>
<ul class="simple"><li>The gradient as a 2D Tensor with shape (nInputs, nOutputs). This is the transpose of the Jacobian.</li>
</ul>

    <a href="https://github.com/SciNim/numericalnim/tree/master/src/numericalnim/differentiate.nim#L93" class="link-seesrc" target="_blank">Source</a>&nbsp;&nbsp;
<a href="https://github.com/SciNim/numericalnim/edit/devel/src/numericalnim/differentiate.nim#L93" class="link-seesrc" target="_blank" >Edit</a>&nbsp;&nbsp;

  </dd>
</div>
<div id="tensorGradient,proc(Tensor[U]),Tensor[U],U,bool">
  <dt><pre><span class="Keyword">proc</span> <a href="#tensorGradient%2Cproc%28Tensor%5BU%5D%29%2CTensor%5BU%5D%2CU%2Cbool"><span class="Identifier">tensorGradient</span></a><span class="Other">[</span><span class="Identifier">U</span><span class="Other">;</span> <span class="Identifier">T</span><span class="Other">:</span> <span class="Keyword">not</span> <span class="Identifier">Tensor</span><span class="Other">]</span><span class="Other">(</span><span class="Identifier">f</span><span class="Other">:</span> <span class="Keyword">proc</span> <span class="Other">(</span><span class="Identifier">x</span><span class="Other">:</span> <span class="Identifier">Tensor</span><span class="Other">[</span><span class="Identifier">U</span><span class="Other">]</span><span class="Other">)</span><span class="Other">:</span> <span class="Identifier">T</span><span class="Other">;</span> <span class="Identifier">x0</span><span class="Other">:</span> <span class="Identifier">Tensor</span><span class="Other">[</span><span class="Identifier">U</span><span class="Other">]</span><span class="Other">;</span>
                                      <span class="Identifier">h</span><span class="Other">:</span> <span class="Identifier">U</span> <span class="Other">=</span> <span class="Identifier">U</span><span class="Other">(</span><span class="FloatNumber">0.000001</span><span class="Other">)</span><span class="Other">;</span> <span class="Identifier">fastMode</span><span class="Other">:</span> <span class="Identifier">bool</span> <span class="Other">=</span> <span class="Identifier">false</span><span class="Other">)</span><span class="Other">:</span> <span class="Identifier">Tensor</span><span class="Other">[</span>
    <span class="Identifier">T</span><span class="Other">]</span></pre></dt>
  <dd>
    
    <p>Calculates the gradient of scalar f(x) w.r.t vector x at x0 using step size h. By default it uses central difference for approximating the derivatives. This requires two function evaluations per derivative. When fastMode is true it will instead use the forward difference which only uses 1 function evaluation per derivative but is less accurate.</p>
<p>Returns:</p>
<ul class="simple"><li>The gradient as a 1D Tensor.</li>
</ul>

    <a href="https://github.com/SciNim/numericalnim/tree/master/src/numericalnim/differentiate.nim#L64" class="link-seesrc" target="_blank">Source</a>&nbsp;&nbsp;
<a href="https://github.com/SciNim/numericalnim/edit/devel/src/numericalnim/differentiate.nim#L64" class="link-seesrc" target="_blank" >Edit</a>&nbsp;&nbsp;

  </dd>
</div>

</div>
<div id="tensorHessian-procs-all">
  <div id="tensorHessian,proc(Tensor[U]),Tensor[U],U">
  <dt><pre><span class="Keyword">proc</span> <a href="#tensorHessian%2Cproc%28Tensor%5BU%5D%29%2CTensor%5BU%5D%2CU"><span class="Identifier">tensorHessian</span></a><span class="Other">[</span><span class="Identifier">U</span><span class="Other">;</span> <span class="Identifier">T</span><span class="Other">:</span> <span class="Keyword">not</span> <span class="Identifier">Tensor</span><span class="Other">]</span><span class="Other">(</span><span class="Identifier">f</span><span class="Other">:</span> <span class="Keyword">proc</span> <span class="Other">(</span><span class="Identifier">x</span><span class="Other">:</span> <span class="Identifier">Tensor</span><span class="Other">[</span><span class="Identifier">U</span><span class="Other">]</span><span class="Other">)</span><span class="Other">:</span> <span class="Identifier">T</span><span class="Other">;</span> <span class="Identifier">x0</span><span class="Other">:</span> <span class="Identifier">Tensor</span><span class="Other">[</span><span class="Identifier">U</span><span class="Other">]</span><span class="Other">;</span>
                                     <span class="Identifier">h</span><span class="Other">:</span> <span class="Identifier">U</span> <span class="Other">=</span> <span class="Identifier">U</span><span class="Other">(</span><span class="FloatNumber">0.000001</span><span class="Other">)</span><span class="Other">)</span><span class="Other">:</span> <span class="Identifier">Tensor</span><span class="Other">[</span><span class="Identifier">T</span><span class="Other">]</span></pre></dt>
  <dd>
    
    <p>Calculates the Hessian of a scalar function f(x) using finite differences at x0. f(x) should accept a 1D Tensor of input values.</p>
<p>Returns:</p>
<ul class="simple"><li>The Hessian as a 2D Tensor of shape (nInputs, nInputs).</li>
</ul>

    <a href="https://github.com/SciNim/numericalnim/tree/master/src/numericalnim/differentiate.nim#L171" class="link-seesrc" target="_blank">Source</a>&nbsp;&nbsp;
<a href="https://github.com/SciNim/numericalnim/edit/devel/src/numericalnim/differentiate.nim#L171" class="link-seesrc" target="_blank" >Edit</a>&nbsp;&nbsp;

  </dd>
</div>

</div>
<div id="tensorJacobian-procs-all">
  <div id="tensorJacobian,proc(Tensor[U]),Tensor[U],U,bool">
  <dt><pre><span class="Keyword">proc</span> <a href="#tensorJacobian%2Cproc%28Tensor%5BU%5D%29%2CTensor%5BU%5D%2CU%2Cbool"><span class="Identifier">tensorJacobian</span></a><span class="Other">[</span><span class="Identifier">U</span><span class="Other">,</span> <span class="Identifier">T</span><span class="Other">]</span><span class="Other">(</span><span class="Identifier">f</span><span class="Other">:</span> <span class="Keyword">proc</span> <span class="Other">(</span><span class="Identifier">x</span><span class="Other">:</span> <span class="Identifier">Tensor</span><span class="Other">[</span><span class="Identifier">U</span><span class="Other">]</span><span class="Other">)</span><span class="Other">:</span> <span class="Identifier">Tensor</span><span class="Other">[</span><span class="Identifier">T</span><span class="Other">]</span><span class="Other">;</span> <span class="Identifier">x0</span><span class="Other">:</span> <span class="Identifier">Tensor</span><span class="Other">[</span><span class="Identifier">U</span><span class="Other">]</span><span class="Other">;</span>
                          <span class="Identifier">h</span><span class="Other">:</span> <span class="Identifier">U</span> <span class="Other">=</span> <span class="Identifier">U</span><span class="Other">(</span><span class="FloatNumber">0.000001</span><span class="Other">)</span><span class="Other">;</span> <span class="Identifier">fastMode</span><span class="Other">:</span> <span class="Identifier">bool</span> <span class="Other">=</span> <span class="Identifier">false</span><span class="Other">)</span><span class="Other">:</span> <span class="Identifier">Tensor</span><span class="Other">[</span><span class="Identifier">T</span><span class="Other">]</span></pre></dt>
  <dd>
    
    Calculates the jacobian of multi-valued f(x) w.r.t vector x at x0 using step size h. f(x) is expected to take as input and return a 1D Tensor. Every row is the gradient of one component of f. By default it uses central difference for approximating the derivatives. This requires two function evaluations per derivative. When fastMode is true it will instead use the forward difference which only uses 1 function evaluation per derivative but is less accurate. Returns:<ul class="simple"><li>The Jacobian as a 2D Tensor with shape (nOutputs, nInputs).</li>
</ul>

    <a href="https://github.com/SciNim/numericalnim/tree/master/src/numericalnim/differentiate.nim#L126" class="link-seesrc" target="_blank">Source</a>&nbsp;&nbsp;
<a href="https://github.com/SciNim/numericalnim/edit/devel/src/numericalnim/differentiate.nim#L126" class="link-seesrc" target="_blank" >Edit</a>&nbsp;&nbsp;

  </dd>
</div>

</div>

  </dl>
</div>

  </div>
</div>

      <div class="twelve-columns footer">
        <span class="nim-sprite"></span>
        <br>
        <small style="color: var(--hint);">Made with Nim. Generated: 2024-09-16 08:27:45 UTC</small>
      </div>
    </div>
  </div>
  
  <!-- Google fonts -->
  <link href='https://fonts.googleapis.com/css?family=Lato:400,600,900' rel='stylesheet' type='text/css'/>
  <link href='https://fonts.googleapis.com/css?family=Source+Code+Pro:400,500,600' rel='stylesheet' type='text/css'/>
</body>
</html>
