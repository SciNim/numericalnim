<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<!--  This file is generated by Nim. -->
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />

<meta name="viewport" content="width=device-width, initial-scale=1.0">

<!-- Favicon -->
<link rel="shortcut icon" href="data:image/x-icon;base64,AAABAAEAEBAAAAEAIABoBAAAFgAAACgAAAAQAAAAIAAAAAEAIAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD///8A////AP///wD///8A////AP///wD///8A////AP///wD///8A////AP///wD///8A////AP///wD///8A////AP///wD///8A////AP///wD///8A////AAAAAAUAAAAF////AP///wD///8A////AP///wD///8A////AP///wD///8A////AAAAAAIAAABbAAAAlQAAAKIAAACbAAAAmwAAAKIAAACVAAAAWwAAAAL///8A////AP///wD///8A////AAAAABQAAADAAAAAYwAAAA3///8A////AP///wD///8AAAAADQAAAGMAAADAAAAAFP///wD///8A////AP///wAAAACdAAAAOv///wD///8A////AP///wD///8A////AP///wD///8AAAAAOgAAAJ3///8A////AP///wAAAAAnAAAAcP///wAAAAAoAAAASv///wD///8A////AP///wAAAABKAAAAKP///wAAAABwAAAAJ////wD///8AAAAAgQAAABwAAACIAAAAkAAAAJMAAACtAAAAFQAAABUAAACtAAAAkwAAAJAAAACIAAAAHAAAAIH///8A////AAAAAKQAAACrAAAAaP///wD///8AAAAARQAAANIAAADSAAAARf///wD///8AAAAAaAAAAKsAAACk////AAAAADMAAACcAAAAnQAAABj///8A////AP///wAAAAAYAAAAGP///wD///8A////AAAAABgAAACdAAAAnAAAADMAAAB1AAAAwwAAAP8AAADpAAAAsQAAAE4AAAAb////AP///wAAAAAbAAAATgAAALEAAADpAAAA/wAAAMMAAAB1AAAAtwAAAOkAAAD/AAAA/wAAAP8AAADvAAAA3gAAAN4AAADeAAAA3gAAAO8AAAD/AAAA/wAAAP8AAADpAAAAtwAAAGUAAAA/AAAA3wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAAD/AAAA/wAAAP8AAADfAAAAPwAAAGX///8A////AAAAAEgAAADtAAAAvwAAAL0AAADGAAAA7wAAAO8AAADGAAAAvQAAAL8AAADtAAAASP///wD///8A////AP///wD///8AAAAAO////wD///8A////AAAAAIcAAACH////AP///wD///8AAAAAO////wD///8A////AP///wD///8A////AP///wD///8A////AP///wD///8A////AP///wD///8A////AP///wD///8A////AP///wD///8A////AP///wD///8A////AP///wD///8A////AP///wD///8A////AP///wD///8A////AP///wD///8A//8AAP//AAD4HwAA7/cAAN/7AAD//wAAoYUAAJ55AACf+QAAh+EAAAAAAADAAwAA4AcAAP5/AAD//wAA//8AAA=="/>
<link rel="icon" type="image/png" sizes="32x32" href="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAABmJLR0QA/wD/AP+gvaeTAAAACXBIWXMAAA3XAAAN1wFCKJt4AAAAB3RJTUUH4QQQEwksSS9ZWwAAAk1JREFUWMPtll2ITVEUx39nn/O7Y5qR8f05wtCUUr6ZIS++8pEnkZInPImneaCQ5METNdOkeFBKUhMPRIkHKfEuUZSUlGlKPN2TrgfncpvmnntnmlEyq1Z7t89/rf9a6+y99oZxGZf/XeIq61EdtgKXgdXA0xrYAvBjOIF1AI9zvjcC74BSpndrJPkBWDScTF8Aa4E3wDlgHbASaANmVqlcCnwHvgDvgVfAJ+AikAAvgfVZwLnSVZHZaOuKoQi3ZOMi4NkYkpe1p4J7A8BpYAD49hfIy/oqG0+hLomiKP2L5L+1ubn5115S+3OAn4EnwBlgMzCjyt6ZAnQCJ4A7wOs88iRJHvw50HoujuPBoCKwHWiosy8MdfZnAdcHk8dxXFJ3VQbQlCTJvRBCGdRbD4M6uc5glpY3eAihpN5S5w12diSEcCCEcKUO4ljdr15T76ur1FDDLIQQ3qv71EdDOe3Kxj3leRXyk+pxdWnFWod6Wt2bY3de3aSuUHcPBVimHs7mK9WrmeOF6lR1o9qnzskh2ar2qm1qizpfXaPeVGdlmGN5pb09qMxz1Xb1kLqgzn1RyH7JUXW52lr5e/Kqi9qpto7V1atuUzfnARrV7jEib1T76gG2qxdGmXyiekkt1GswPTtek0aBfJp6YySGBfWg2tPQ0FAYgf1stUfdmdcjarbYJEniKIq6gY/Aw+zWHAC+p2labGpqiorFYgGYCEzN7oQdQClN07O1/EfDyGgC0ALMBdYAi4FyK+4H3gLPsxfR1zRNi+NP7nH5J+QntnXe5B5mpfQAAAAASUVORK5CYII=">

<!-- Google fonts -->
<link href='https://fonts.googleapis.com/css?family=Lato:400,600,900' rel='stylesheet' type='text/css'/>
<link href='https://fonts.googleapis.com/css?family=Source+Code+Pro:400,500,600' rel='stylesheet' type='text/css'/>

<!-- CSS -->
<title>src/numericalnim/optimize</title>
<link rel="stylesheet" type="text/css" href="../nimdoc.out.css">

<script type="text/javascript" src="../dochack.js"></script>

<script type="text/javascript">
function main() {
  var pragmaDots = document.getElementsByClassName("pragmadots");
  for (var i = 0; i < pragmaDots.length; i++) {
    pragmaDots[i].onclick = function(event) {
      // Hide tease
      event.target.parentNode.style.display = "none";
      // Show actual
      event.target.parentNode.nextElementSibling.style.display = "inline";
    }
  }

  function switchTheme(e) {
      if (e.target.checked) {
          document.documentElement.setAttribute('data-theme', 'dark');
          localStorage.setItem('theme', 'dark');
      } else {
          document.documentElement.setAttribute('data-theme', 'light');
          localStorage.setItem('theme', 'light');
      }
  }

  const toggleSwitch = document.querySelector('.theme-switch input[type="checkbox"]');
  if (toggleSwitch !== null) {
    toggleSwitch.addEventListener('change', switchTheme, false);
  }

  var currentTheme = localStorage.getItem('theme');
  if (!currentTheme && window.matchMedia('(prefers-color-scheme: dark)').matches) {
    currentTheme = 'dark';
  }
  if (currentTheme) {
    document.documentElement.setAttribute('data-theme', currentTheme);

    if (currentTheme === 'dark' && toggleSwitch !== null) {
      toggleSwitch.checked = true;
    }
  }
}

window.addEventListener('DOMContentLoaded', main);
</script>

</head>
<body>
<div class="document" id="documentId">
  <div class="container">
    <h1 class="title">src/numericalnim/optimize</h1>
    <div class="row">
  <div class="three columns">
  <div class="theme-switch-wrapper">
    <label class="theme-switch" for="checkbox">
      <input type="checkbox" id="checkbox" />
      <div class="slider round"></div>
    </label>
    &nbsp;&nbsp;&nbsp; <em>Dark Mode</em>
  </div>
  <div id="global-links">
    <ul class="simple">
    <li>
      <a href="../theindex.html">Index</a>
    </li>
    </ul>
  </div>
  <div id="searchInputDiv">
    Search: <input type="text" id="searchInput"
      onkeyup="search()" />
  </div>
  <div>
    Group by:
    <select onchange="groupBy(this.value)">
      <option value="section">Section</option>
      <option value="type">Type</option>
    </select>
  </div>
  <ul class="simple simple-toc" id="toc-list">
<li><a class="reference" id="optimization_toc" href="#optimization">Optimization</a></li>
<ul class="simple"><li><a class="reference" id="optimization-optimization_toc" href="#optimization-optimization">Optimization</a></li>
<li><a class="reference" id="optimization-curve-fitting_toc" href="#optimization-curve-fitting">Curve fitting</a></li>
</ul><li>
  <a class="reference reference-toplevel" href="#6" id="56">Imports</a>
  <ul class="simple simple-toc-section">
    
  </ul>
</li>
<li>
  <a class="reference reference-toplevel" href="#7" id="57">Types</a>
  <ul class="simple simple-toc-section">
      <li><a class="reference" href="#LBFGSOptions"
    title="LBFGSOptions[U] = object
  savedIterations*: int">LBFGSOptions</a></li>
  <li><a class="reference" href="#LevMarqOptions"
    title="LevMarqOptions[U] = object
  lambda0*: U">LevMarqOptions</a></li>
  <li><a class="reference" href="#LineSearchCriterion"
    title="LineSearchCriterion = enum
  Armijo, Wolfe, WolfeStrong, NoLineSearch">LineSearchCriterion</a></li>
  <li><a class="reference" href="#OptimOptions"
    title="OptimOptions[U; AO] = object
  tol*, alpha*: U
  fastMode*: bool
  maxIterations*: int
  lineSearchCriterion*: LineSearchCriterion
  algoOptions*: AO">OptimOptions</a></li>
  <li><a class="reference" href="#StandardOptions"
    title="StandardOptions = object">StandardOptions</a></li>

  </ul>
</li>
<li>
  <a class="reference reference-toplevel" href="#12" id="62">Procs</a>
  <ul class="simple simple-toc-section">
      <ul class="simple nested-toc-section">bfgs
      <li><a class="reference" href="#bfgs%2Cproc%28Tensor%5BU%5D%29%2CTensor%5BU%5D%2COptimOptions%5BU%2CStandardOptions%5D%2Cproc%28Tensor%5BU%5D%29"
    title="bfgs[U; T: not Tensor](f: proc (x: Tensor[U]): T; x0: Tensor[U]; options: OptimOptions[
    U, StandardOptions] = bfgsOptions[U]();
                       analyticGradient: proc (x: Tensor[U]): Tensor[T] = nil): Tensor[
    U]">bfgs[U; T: not Tensor](f: proc (x: Tensor[U]): T; x0: Tensor[U]; options: OptimOptions[
    U, StandardOptions] = bfgsOptions[U]();
                       analyticGradient: proc (x: Tensor[U]): Tensor[T] = nil): Tensor[
    U]</a></li>

  </ul>
  <ul class="simple nested-toc-section">bfgs_old
      <li><a class="reference" href="#bfgs_old%2Cproc%28Tensor%5BU%5D%29%2CTensor%5BU%5D%2CU%2CU%2Cbool%2Cproc%28Tensor%5BU%5D%29"
    title="bfgs_old[U; T: not Tensor](f: proc (x: Tensor[U]): T; x0: Tensor[U];
                           alpha: U = U(1); tol: U = U(0.000001);
                           fastMode: bool = false; analyticGradient: proc (
    x: Tensor[U]): Tensor[T] = nil): Tensor[U]">bfgs_old[U; T: not Tensor](f: proc (x: Tensor[U]): T; x0: Tensor[U];
                           alpha: U = U(1); tol: U = U(0.000001);
                           fastMode: bool = false; analyticGradient: proc (
    x: Tensor[U]): Tensor[T] = nil): Tensor[U]</a></li>

  </ul>
  <ul class="simple nested-toc-section">bfgsOptions
      <li><a class="reference" href="#bfgsOptions%2CU%2CU%2Cbool%2Cint%2CLineSearchCriterion"
    title="bfgsOptions[U](tol: U = U(0.000001); alpha: U = U(1); fastMode: bool = false;
               maxIterations: int = 10000;
               lineSearchCriterion: LineSearchCriterion = NoLineSearch): OptimOptions[
    U, StandardOptions]">bfgsOptions[U](tol: U = U(0.000001); alpha: U = U(1); fastMode: bool = false;
               maxIterations: int = 10000;
               lineSearchCriterion: LineSearchCriterion = NoLineSearch): OptimOptions[
    U, StandardOptions]</a></li>

  </ul>
  <ul class="simple nested-toc-section">conjugate_gradient
      <li><a class="reference" href="#conjugate_gradient%2CTensor%5BT%5D%2CTensor%5BT%5D%2CTensor%5BT%5D%2Cfloat64"
    title="conjugate_gradient[T](A, b, x_0: Tensor[T]; tolerance: float64): Tensor[T]">conjugate_gradient[T](A, b, x_0: Tensor[T]; tolerance: float64): Tensor[T]</a></li>

  </ul>
  <ul class="simple nested-toc-section">lbfgs
      <li><a class="reference" href="#lbfgs%2Cproc%28Tensor%5BU%5D%29%2CTensor%5BU%5D%2COptimOptions%5BU%2CLBFGSOptions%5BU%5D%5D%2Cproc%28Tensor%5BU%5D%29"
    title="lbfgs[U; T: not Tensor](f: proc (x: Tensor[U]): T; x0: Tensor[U]; options: OptimOptions[
    U, LBFGSOptions[U]] = lbfgsOptions[U]();
                        analyticGradient: proc (x: Tensor[U]): Tensor[T] = nil): Tensor[
    U]">lbfgs[U; T: not Tensor](f: proc (x: Tensor[U]): T; x0: Tensor[U]; options: OptimOptions[
    U, LBFGSOptions[U]] = lbfgsOptions[U]();
                        analyticGradient: proc (x: Tensor[U]): Tensor[T] = nil): Tensor[
    U]</a></li>

  </ul>
  <ul class="simple nested-toc-section">lbfgsOptions
      <li><a class="reference" href="#lbfgsOptions%2Cint%2CU%2CU%2Cbool%2Cint%2CLineSearchCriterion"
    title="lbfgsOptions[U](savedIterations: int = 10; tol: U = U(0.000001);
                alpha: U = U(1); fastMode: bool = false;
                maxIterations: int = 10000;
                lineSearchCriterion: LineSearchCriterion = NoLineSearch): OptimOptions[
    U, LBFGSOptions[U]]">lbfgsOptions[U](savedIterations: int = 10; tol: U = U(0.000001);
                alpha: U = U(1); fastMode: bool = false;
                maxIterations: int = 10000;
                lineSearchCriterion: LineSearchCriterion = NoLineSearch): OptimOptions[
    U, LBFGSOptions[U]]</a></li>

  </ul>
  <ul class="simple nested-toc-section">levmarq
      <li><a class="reference" href="#levmarq%2Cproc%28Tensor%5BU%5D%2CU%29%2CTensor%5BU%5D%2CTensor%5BU%5D%2CTensor%5BT%3A%20not%20Tensor%5D%2COptimOptions%5BU%2CLevMarqOptions%5BU%5D%5D%2CTensor%5BT%3A%20not%20Tensor%5D"
    title="levmarq[U; T: not Tensor](f: proc (params: Tensor[U]; x: U): T;
                          params0: Tensor[U]; xData: Tensor[U];
                          yData: Tensor[T]; options: OptimOptions[U,
    LevMarqOptions[U]] = levmarqOptions[U]();
                          yError: Tensor[T] = ones_like(yData)): Tensor[U]">levmarq[U; T: not Tensor](f: proc (params: Tensor[U]; x: U): T;
                          params0: Tensor[U]; xData: Tensor[U];
                          yData: Tensor[T]; options: OptimOptions[U,
    LevMarqOptions[U]] = levmarqOptions[U]();
                          yError: Tensor[T] = ones_like(yData)): Tensor[U]</a></li>

  </ul>
  <ul class="simple nested-toc-section">levmarqOptions
      <li><a class="reference" href="#levmarqOptions%2CU%2CU%2CU%2Cbool%2Cint%2CLineSearchCriterion"
    title="levmarqOptions[U](lambda0: U = U(1); tol: U = U(0.000001); alpha: U = U(1);
                  fastMode: bool = false; maxIterations: int = 10000;
                  lineSearchCriterion: LineSearchCriterion = NoLineSearch): OptimOptions[
    U, LevMarqOptions[U]]">levmarqOptions[U](lambda0: U = U(1); tol: U = U(0.000001); alpha: U = U(1);
                  fastMode: bool = false; maxIterations: int = 10000;
                  lineSearchCriterion: LineSearchCriterion = NoLineSearch): OptimOptions[
    U, LevMarqOptions[U]]</a></li>

  </ul>
  <ul class="simple nested-toc-section">line_search
      <li><a class="reference" href="#line_search%2CU%2CTensor%5BT%5D%2CTensor%5BU%5D%2Cproc%28Tensor%5BU%5D%29%2CLineSearchCriterion%2Cbool"
    title="line_search[U, T](alpha: var U; p: Tensor[T]; x0: Tensor[U];
                  f: proc (x: Tensor[U]): T; criterion: LineSearchCriterion;
                  fastMode: bool = false)">line_search[U, T](alpha: var U; p: Tensor[T]; x0: Tensor[U];
                  f: proc (x: Tensor[U]): T; criterion: LineSearchCriterion;
                  fastMode: bool = false)</a></li>

  </ul>
  <ul class="simple nested-toc-section">newton
      <li><a class="reference" href="#newton%2Cproc%28Tensor%5BU%5D%29%2CTensor%5BU%5D%2COptimOptions%5BU%2CStandardOptions%5D%2Cproc%28Tensor%5BU%5D%29"
    title="newton[U; T: not Tensor](f: proc (x: Tensor[U]): T; x0: Tensor[U]; options: OptimOptions[
    U, StandardOptions] = newtonOptions[U]();
                         analyticGradient: proc (x: Tensor[U]): Tensor[T] = nil): Tensor[
    U]">newton[U; T: not Tensor](f: proc (x: Tensor[U]): T; x0: Tensor[U]; options: OptimOptions[
    U, StandardOptions] = newtonOptions[U]();
                         analyticGradient: proc (x: Tensor[U]): Tensor[T] = nil): Tensor[
    U]</a></li>

  </ul>
  <ul class="simple nested-toc-section">newtonOptions
      <li><a class="reference" href="#newtonOptions%2CU%2CU%2Cbool%2Cint%2CLineSearchCriterion"
    title="newtonOptions[U](tol: U = U(0.000001); alpha: U = U(1); fastMode: bool = false;
                 maxIterations: int = 10000;
                 lineSearchCriterion: LineSearchCriterion = NoLineSearch): OptimOptions[
    U, StandardOptions]">newtonOptions[U](tol: U = U(0.000001); alpha: U = U(1); fastMode: bool = false;
                 maxIterations: int = 10000;
                 lineSearchCriterion: LineSearchCriterion = NoLineSearch): OptimOptions[
    U, StandardOptions]</a></li>

  </ul>
  <ul class="simple nested-toc-section">newtons
      <li><a class="reference" href="#newtons%2Cproc%28float64%29%2Cproc%28float64%29%2Cfloat64%2Cfloat64%2CNatural"
    title="newtons(f: proc (x: float64): float64; deriv: proc (x: float64): float64;
        start: float64; precision: float64 = 0.00001; max_iters: Natural = 1000): float64">newtons(f: proc (x: float64): float64; deriv: proc (x: float64): float64;
        start: float64; precision: float64 = 0.00001; max_iters: Natural = 1000): float64</a></li>

  </ul>
  <ul class="simple nested-toc-section">optimOptions
      <li><a class="reference" href="#optimOptions%2CU%2CU%2Cbool%2Cint%2CLineSearchCriterion"
    title="optimOptions[U](tol: U = U(0.000001); alpha: U = U(1); fastMode: bool = false;
                maxIterations: int = 10000;
                lineSearchCriterion: LineSearchCriterion = NoLineSearch): OptimOptions[
    U, StandardOptions]">optimOptions[U](tol: U = U(0.000001); alpha: U = U(1); fastMode: bool = false;
                maxIterations: int = 10000;
                lineSearchCriterion: LineSearchCriterion = NoLineSearch): OptimOptions[
    U, StandardOptions]</a></li>

  </ul>
  <ul class="simple nested-toc-section">paramUncertainties
      <li><a class="reference" href="#paramUncertainties%2CTensor%5BU%5D%2Cproc%28Tensor%5BU%5D%2CU%29%2CTensor%5BU%5D%2CTensor%5BT%5D%2CTensor%5BT%5D"
    title="paramUncertainties[U; T](params: Tensor[U];
                         fitFunc: proc (params: Tensor[U]; x: U): T;
                         xData: Tensor[U]; yData: Tensor[T]; yError: Tensor[T];
                         returnFullCov = false): Tensor[T]">paramUncertainties[U; T](params: Tensor[U];
                         fitFunc: proc (params: Tensor[U]; x: U): T;
                         xData: Tensor[U]; yData: Tensor[T]; yError: Tensor[T];
                         returnFullCov = false): Tensor[T]</a></li>

  </ul>
  <ul class="simple nested-toc-section">secant
      <li><a class="reference" href="#secant%2Cproc%28float64%29%2Carray%5B%2Cfloat64%5D%2Cfloat64%2CNatural"
    title="secant(f: proc (x: float64): float64; start: array[2, float64];
       precision: float64 = 0.00001; max_iters: Natural = 1000): float64">secant(f: proc (x: float64): float64; start: array[2, float64];
       precision: float64 = 0.00001; max_iters: Natural = 1000): float64</a></li>

  </ul>
  <ul class="simple nested-toc-section">steepest_descent
      <li><a class="reference" href="#steepest_descent%2Cproc%28float64%29%2Cfloat64%2Cfloat64%2Cfloat64%2CNatural"
    title="steepest_descent(deriv: proc (x: float64): float64; start: float64;
                 gamma: float64 = 0.01; precision: float64 = 0.00001;
                 max_iters: Natural = 1000): float64">steepest_descent(deriv: proc (x: float64): float64; start: float64;
                 gamma: float64 = 0.01; precision: float64 = 0.00001;
                 max_iters: Natural = 1000): float64</a></li>

  </ul>
  <ul class="simple nested-toc-section">steepestDescent
      <li><a class="reference" href="#steepestDescent%2Cproc%28Tensor%5BU%5D%29%2CTensor%5BU%5D%2COptimOptions%5BU%2CStandardOptions%5D%2Cproc%28Tensor%5BU%5D%29"
    title="steepestDescent[U; T: not Tensor](f: proc (x: Tensor[U]): T; x0: Tensor[U];
    options: OptimOptions[U, StandardOptions] = steepestDescentOptions[U]();
    analyticGradient: proc (x: Tensor[U]): Tensor[T] = nil): Tensor[U]">steepestDescent[U; T: not Tensor](f: proc (x: Tensor[U]): T; x0: Tensor[U];
    options: OptimOptions[U, StandardOptions] = steepestDescentOptions[U]();
    analyticGradient: proc (x: Tensor[U]): Tensor[T] = nil): Tensor[U]</a></li>

  </ul>
  <ul class="simple nested-toc-section">steepestDescentOptions
      <li><a class="reference" href="#steepestDescentOptions%2CU%2CU%2Cbool%2Cint%2CLineSearchCriterion"
    title="steepestDescentOptions[U](tol: U = U(0.000001); alpha: U = U(0.001);
                          fastMode: bool = false; maxIterations: int = 10000;
    lineSearchCriterion: LineSearchCriterion = NoLineSearch): OptimOptions[U,
    StandardOptions]">steepestDescentOptions[U](tol: U = U(0.000001); alpha: U = U(0.001);
                          fastMode: bool = false; maxIterations: int = 10000;
    lineSearchCriterion: LineSearchCriterion = NoLineSearch): OptimOptions[U,
    StandardOptions]</a></li>

  </ul>
  <ul class="simple nested-toc-section">vectorNorm
      <li><a class="reference" href="#vectorNorm%2CTensor%5BT%5D"
    title="vectorNorm[T](v: Tensor[T]): T">vectorNorm[T](v: Tensor[T]): T</a></li>

  </ul>

  </ul>
</li>

</ul>

  </div>
  &nbsp;&nbsp;<a
href="https://github.com/SciNim/numericalnim/tree/master/src/numericalnim/optimize.nim#L1"
class="link-seesrc" target="_blank">Source</a>
&nbsp;&nbsp;<a href="https://github.com/SciNim/numericalnim/edit/devel/src/numericalnim/optimize.nim#L1" class="link-seesrc" target="_blank" >Edit</a>

  <div class="nine columns" id="content">
  <div id="tocRoot"></div>
  
  <p class="module-desc">
<h1><a class="toc-backref" id="optimization" href="#optimization">Optimization</a></h1><p>This module implements optimization and curve fitting routines.</p>

<h2><a class="toc-backref" id="optimization-optimization" href="#optimization-optimization">Optimization</a></h2><p>These optimization methods are provided:</p>
<ul class="simple"><li><tt class="docutils literal"><span class="pre"><span class="Identifier">lbfgs</span></span></tt> (recommended): A quasi-Newton method that strikes a good balance between accuracy and performance.</li>
<li><tt class="docutils literal"><span class="pre"><span class="Identifier">bfgs</span></span></tt>: A quasi-Newton method. <tt class="docutils literal"><span class="pre"><span class="Identifier">lbfgs</span></span></tt> is a lighter version of this.</li>
<li><tt class="docutils literal"><span class="pre"><span class="Identifier">newton</span></span></tt>: Newton's method. Fast for small problems but struggles when the number of variables increase.</li>
<li><tt class="docutils literal"><span class="pre"><span class="Identifier">steepestDescent</span></span></tt>: The classical gradient descent. Has slow convergence compared to the others.</li>
</ul>
<p>By default the gradients are approximated using finite differences. Optionally an analytical gradient can be supplied with the <tt class="docutils literal"><span class="pre"><span class="Identifier">analyticGradient</span></span></tt> argument.</p>

<p><strong class="examples_text">Example:</strong></p>
<pre class="listing"><span class="Keyword">import</span> <span class="Identifier">src</span><span class="Operator">/</span><span class="Identifier">numericalnim</span><span class="Operator">/</span><span class="Identifier">optimize</span>
<span class="Keyword">import</span> <span class="Identifier">arraymancer</span>
<span class="Comment"># f(x, y) = x^2 + y^2</span>
<span class="Keyword">proc</span> <span class="Identifier">f</span><span class="Punctuation">(</span><span class="Identifier">x</span><span class="Punctuation">:</span> <span class="Identifier">Tensor</span><span class="Punctuation">[</span><span class="Identifier">float</span><span class="Punctuation">]</span><span class="Punctuation">)</span><span class="Punctuation">:</span> <span class="Identifier">float</span> <span class="Operator">=</span>
    <span class="Identifier">x</span><span class="Punctuation">[</span><span class="DecNumber">0</span><span class="Punctuation">]</span><span class="Operator">*</span><span class="Identifier">x</span><span class="Punctuation">[</span><span class="DecNumber">0</span><span class="Punctuation">]</span> <span class="Operator">+</span> <span class="Identifier">x</span><span class="Punctuation">[</span><span class="DecNumber">1</span><span class="Punctuation">]</span><span class="Operator">*</span><span class="Identifier">x</span><span class="Punctuation">[</span><span class="DecNumber">1</span><span class="Punctuation">]</span>

<span class="Keyword">let</span> <span class="Identifier">guess</span> <span class="Operator">=</span> <span class="Punctuation">[</span><span class="FloatNumber">1.0</span><span class="Punctuation">,</span> <span class="FloatNumber">1.0</span><span class="Punctuation">]</span><span class="Operator">.</span><span class="Identifier">toTensor</span>
<span class="Keyword">let</span> <span class="Identifier">sol</span> <span class="Operator">=</span> <span class="Identifier">lbfgs</span><span class="Punctuation">(</span><span class="Identifier">f</span><span class="Punctuation">,</span> <span class="Identifier">guess</span><span class="Punctuation">)</span></pre>
<h2><a class="toc-backref" id="optimization-curve-fitting" href="#optimization-curve-fitting">Curve fitting</a></h2><p>A Levenberg-Marquardt non-linear least squares solver is provided in <tt class="docutils literal"><span class="pre"><span class="Identifier">levmarq</span></span></tt>. The curve to fit is provided as a function taking in a 1D <tt class="docutils literal"><span class="pre"><span class="Identifier">Tensor</span></span></tt> with all parameters and the value of the independent variable to evaluate it at.</p>
<p>Optionally the <tt class="docutils literal"><span class="pre"><span class="Identifier">y</span></span></tt>-errors can be supplied to the <tt class="docutils literal"><span class="pre"><span class="Identifier">yError</span></span></tt> argument to take the uncertainties of each of the points into account. The uncertainties of the fitted parameters can be obtained by calling <tt class="docutils literal"><span class="pre"><span class="Identifier">paramUncertainties</span></span></tt>.</p>

<p><strong class="examples_text">Example:</strong></p>
<pre class="listing"><span class="Keyword">import</span> <span class="Identifier">src</span><span class="Operator">/</span><span class="Identifier">numericalnim</span><span class="Operator">/</span><span class="Identifier">optimize</span>
<span class="Keyword">import</span> <span class="Identifier">arraymancer</span>
<span class="Comment"># f(x) = a*x + b</span>
<span class="Keyword">proc</span> <span class="Identifier">f</span><span class="Punctuation">(</span><span class="Identifier">params</span><span class="Punctuation">:</span> <span class="Identifier">Tensor</span><span class="Punctuation">[</span><span class="Identifier">float</span><span class="Punctuation">]</span><span class="Punctuation">,</span> <span class="Identifier">x</span><span class="Punctuation">:</span> <span class="Identifier">float</span><span class="Punctuation">)</span><span class="Punctuation">:</span> <span class="Identifier">float</span> <span class="Operator">=</span>
    <span class="Keyword">let</span> <span class="Identifier">a</span> <span class="Operator">=</span> <span class="Identifier">params</span><span class="Punctuation">[</span><span class="DecNumber">0</span><span class="Punctuation">]</span>
    <span class="Keyword">let</span> <span class="Identifier">b</span> <span class="Operator">=</span> <span class="Identifier">params</span><span class="Punctuation">[</span><span class="DecNumber">1</span><span class="Punctuation">]</span>
    <span class="Identifier">result</span> <span class="Operator">=</span> <span class="Identifier">a</span><span class="Operator">*</span><span class="Identifier">x</span> <span class="Operator">+</span> <span class="Identifier">b</span>

<span class="Comment"># Generate measurements</span>
<span class="Keyword">let</span> <span class="Identifier">x</span> <span class="Operator">=</span> <span class="Identifier">linspace</span><span class="Punctuation">(</span><span class="FloatNumber">0.0</span><span class="Punctuation">,</span> <span class="FloatNumber">10.0</span><span class="Punctuation">,</span> <span class="DecNumber">10</span><span class="Punctuation">)</span>
<span class="Keyword">let</span> <span class="Identifier">y</span> <span class="Operator">=</span> <span class="FloatNumber">3.14</span> <span class="Operator">*</span> <span class="Identifier">x</span> <span class="Operator">+.</span> <span class="FloatNumber">2.81</span> <span class="Operator">+</span> <span class="Identifier">randomNormalTensor</span><span class="Punctuation">(</span><span class="DecNumber">10</span><span class="Punctuation">,</span> <span class="FloatNumber">0.0</span><span class="Punctuation">,</span> <span class="FloatNumber">0.05</span><span class="Punctuation">)</span>
<span class="Keyword">let</span> <span class="Identifier">yError</span> <span class="Operator">=</span> <span class="FloatNumber">0.05</span> <span class="Operator">*</span> <span class="Identifier">ones</span><span class="Punctuation">[</span><span class="Identifier">float</span><span class="Punctuation">]</span><span class="Punctuation">(</span><span class="DecNumber">10</span><span class="Punctuation">)</span>
<span class="Comment"># Solve for best fit parameters</span>
<span class="Keyword">let</span> <span class="Identifier">guess</span> <span class="Operator">=</span> <span class="Punctuation">[</span><span class="FloatNumber">1.0</span><span class="Punctuation">,</span> <span class="FloatNumber">1.0</span><span class="Punctuation">]</span><span class="Operator">.</span><span class="Identifier">toTensor</span>
<span class="Keyword">let</span> <span class="Identifier">solution</span> <span class="Operator">=</span> <span class="Identifier">levmarq</span><span class="Punctuation">(</span><span class="Identifier">f</span><span class="Punctuation">,</span> <span class="Identifier">guess</span><span class="Punctuation">,</span> <span class="Identifier">x</span><span class="Punctuation">,</span> <span class="Identifier">y</span><span class="Punctuation">,</span> <span class="Identifier">yError</span><span class="Operator">=</span><span class="Identifier">yError</span><span class="Punctuation">)</span>
<span class="Comment"># Calculate uncertainties in fitted parameters</span>
<span class="Keyword">let</span> <span class="Identifier">uncertainties</span> <span class="Operator">=</span> <span class="Identifier">sqrt</span><span class="Punctuation">(</span><span class="Identifier">paramUncertainties</span><span class="Punctuation">(</span><span class="Identifier">solution</span><span class="Punctuation">,</span> <span class="Identifier">f</span><span class="Punctuation">,</span> <span class="Identifier">x</span><span class="Punctuation">,</span> <span class="Identifier">y</span><span class="Punctuation">,</span> <span class="Identifier">yError</span><span class="Punctuation">)</span><span class="Punctuation">)</span></pre></p>
  <div class="section" id="6">
<h1><a class="toc-backref" href="#6">Imports</a></h1>
<dl class="item">
<a class="reference external" href="differentiate.html">differentiate</a>, <a class="reference external" href="utils.html">utils</a>
</dl></div>
<div class="section" id="7">
<h1><a class="toc-backref" href="#7">Types</a></h1>
<dl class="item">
<div id="LBFGSOptions">
<dt><pre><a href="optimize.html#LBFGSOptions"><span class="Identifier">LBFGSOptions</span></a><span class="Other">[</span><span class="Identifier">U</span><span class="Other">]</span> <span class="Other">=</span> <span class="Keyword">object</span>
  <span class="Identifier">savedIterations</span><span class="Operator">*</span><span class="Other">:</span> <span class="Identifier">int</span>
</pre></dt>
<dd>


&nbsp;&nbsp;<a
href="https://github.com/SciNim/numericalnim/tree/master/src/numericalnim/optimize.nim#L200"
class="link-seesrc" target="_blank">Source</a>
&nbsp;&nbsp;<a href="https://github.com/SciNim/numericalnim/edit/devel/src/numericalnim/optimize.nim#L200" class="link-seesrc" target="_blank" >Edit</a>

</dd>
</div>
<div id="LevMarqOptions">
<dt><pre><a href="optimize.html#LevMarqOptions"><span class="Identifier">LevMarqOptions</span></a><span class="Other">[</span><span class="Identifier">U</span><span class="Other">]</span> <span class="Other">=</span> <span class="Keyword">object</span>
  <span class="Identifier">lambda0</span><span class="Operator">*</span><span class="Other">:</span> <span class="Identifier">U</span>
</pre></dt>
<dd>


&nbsp;&nbsp;<a
href="https://github.com/SciNim/numericalnim/tree/master/src/numericalnim/optimize.nim#L198"
class="link-seesrc" target="_blank">Source</a>
&nbsp;&nbsp;<a href="https://github.com/SciNim/numericalnim/edit/devel/src/numericalnim/optimize.nim#L198" class="link-seesrc" target="_blank" >Edit</a>

</dd>
</div>
<div id="LineSearchCriterion">
<dt><pre><a href="optimize.html#LineSearchCriterion"><span class="Identifier">LineSearchCriterion</span></a> <span class="Other">=</span> <span class="Keyword">enum</span>
  <span class="Identifier">Armijo</span><span class="Other">,</span> <span class="Identifier">Wolfe</span><span class="Other">,</span> <span class="Identifier">WolfeStrong</span><span class="Other">,</span> <span class="Identifier">NoLineSearch</span></pre></dt>
<dd>


&nbsp;&nbsp;<a
href="https://github.com/SciNim/numericalnim/tree/master/src/numericalnim/optimize.nim#L187"
class="link-seesrc" target="_blank">Source</a>
&nbsp;&nbsp;<a href="https://github.com/SciNim/numericalnim/edit/devel/src/numericalnim/optimize.nim#L187" class="link-seesrc" target="_blank" >Edit</a>

</dd>
</div>
<div id="OptimOptions">
<dt><pre><a href="optimize.html#OptimOptions"><span class="Identifier">OptimOptions</span></a><span class="Other">[</span><span class="Identifier">U</span><span class="Other">;</span> <span class="Identifier">AO</span><span class="Other">]</span> <span class="Other">=</span> <span class="Keyword">object</span>
  <span class="Identifier">tol</span><span class="Operator">*</span><span class="Other">,</span> <span class="Identifier">alpha</span><span class="Operator">*</span><span class="Other">:</span> <span class="Identifier">U</span>
  <span class="Identifier">fastMode</span><span class="Operator">*</span><span class="Other">:</span> <span class="Identifier">bool</span>
  <span class="Identifier">maxIterations</span><span class="Operator">*</span><span class="Other">:</span> <span class="Identifier">int</span>
  <span class="Identifier">lineSearchCriterion</span><span class="Operator">*</span><span class="Other">:</span> <a href="optimize.html#LineSearchCriterion"><span class="Identifier">LineSearchCriterion</span></a>
  <span class="Identifier">algoOptions</span><span class="Operator">*</span><span class="Other">:</span> <span class="Identifier">AO</span>
</pre></dt>
<dd>


&nbsp;&nbsp;<a
href="https://github.com/SciNim/numericalnim/tree/master/src/numericalnim/optimize.nim#L191"
class="link-seesrc" target="_blank">Source</a>
&nbsp;&nbsp;<a href="https://github.com/SciNim/numericalnim/edit/devel/src/numericalnim/optimize.nim#L191" class="link-seesrc" target="_blank" >Edit</a>

</dd>
</div>
<div id="StandardOptions">
<dt><pre><a href="optimize.html#StandardOptions"><span class="Identifier">StandardOptions</span></a> <span class="Other">=</span> <span class="Keyword">object</span></pre></dt>
<dd>


&nbsp;&nbsp;<a
href="https://github.com/SciNim/numericalnim/tree/master/src/numericalnim/optimize.nim#L197"
class="link-seesrc" target="_blank">Source</a>
&nbsp;&nbsp;<a href="https://github.com/SciNim/numericalnim/edit/devel/src/numericalnim/optimize.nim#L197" class="link-seesrc" target="_blank" >Edit</a>

</dd>
</div>

</dl></div>
<div class="section" id="12">
<h1><a class="toc-backref" href="#12">Procs</a></h1>
<dl class="item">
<div id="bfgs,proc(Tensor[U]),Tensor[U],OptimOptions[U,StandardOptions],proc(Tensor[U])">
<dt><pre><span class="Keyword">proc</span> <a href="#bfgs%2Cproc%28Tensor%5BU%5D%29%2CTensor%5BU%5D%2COptimOptions%5BU%2CStandardOptions%5D%2Cproc%28Tensor%5BU%5D%29"><span class="Identifier">bfgs</span></a><span class="Other">[</span><span class="Identifier">U</span><span class="Other">;</span> <span class="Identifier">T</span><span class="Other">:</span> <span class="Keyword">not</span> <span class="Identifier">Tensor</span><span class="Other">]</span><span class="Other">(</span><span class="Identifier">f</span><span class="Other">:</span> <span class="Keyword">proc</span> <span class="Other">(</span><span class="Identifier">x</span><span class="Other">:</span> <span class="Identifier">Tensor</span><span class="Other">[</span><span class="Identifier">U</span><span class="Other">]</span><span class="Other">)</span><span class="Other">:</span> <span class="Identifier">T</span><span class="Other">;</span> <span class="Identifier">x0</span><span class="Other">:</span> <span class="Identifier">Tensor</span><span class="Other">[</span><span class="Identifier">U</span><span class="Other">]</span><span class="Other">;</span> <span class="Identifier">options</span><span class="Other">:</span> <a href="optimize.html#OptimOptions"><span class="Identifier">OptimOptions</span></a><span class="Other">[</span>
    <span class="Identifier">U</span><span class="Other">,</span> <a href="optimize.html#StandardOptions"><span class="Identifier">StandardOptions</span></a><span class="Other">]</span> <span class="Other">=</span> <span class="Identifier">bfgsOptions</span><span class="Other">[</span><span class="Identifier">U</span><span class="Other">]</span><span class="Other">(</span><span class="Other">)</span><span class="Other">;</span> <span class="Identifier">analyticGradient</span><span class="Other">:</span> <span class="Keyword">proc</span> <span class="Other">(</span><span class="Identifier">x</span><span class="Other">:</span> <span class="Identifier">Tensor</span><span class="Other">[</span><span class="Identifier">U</span><span class="Other">]</span><span class="Other">)</span><span class="Other">:</span> <span class="Identifier">Tensor</span><span class="Other">[</span>
    <span class="Identifier">T</span><span class="Other">]</span> <span class="Other">=</span> <span class="Keyword">nil</span><span class="Other">)</span><span class="Other">:</span> <span class="Identifier">Tensor</span><span class="Other">[</span><span class="Identifier">U</span><span class="Other">]</span></pre></dt>
<dd>

<p>BFGS (Broyden–Fletcher–Goldfarb–Shanno) method for optimization.</p>
<p>Inputs:</p>
<ul class="simple"><li>f: The function to optimize. It should take as input a 1D Tensor of the input variables and return a scalar.</li>
<li>options: Options object (see <tt class="docutils literal"><span class="pre"><span class="Identifier">bfgsOptions</span></span></tt> for constructing one)</li>
<li>analyticGradient: The analytic gradient of <tt class="docutils literal"><span class="pre"><span class="Identifier">f</span></span></tt> taking in and returning a 1D Tensor. If not provided, a finite difference approximation will be performed instead.</li>
</ul>
<p>Returns:</p>
<ul class="simple"><li>The final solution for the parameters. Either because a (local) minimum was found or because the maximum number of iterations was reached.</li>
</ul>

&nbsp;&nbsp;<a
href="https://github.com/SciNim/numericalnim/tree/master/src/numericalnim/optimize.nim#L458"
class="link-seesrc" target="_blank">Source</a>
&nbsp;&nbsp;<a href="https://github.com/SciNim/numericalnim/edit/devel/src/numericalnim/optimize.nim#L458" class="link-seesrc" target="_blank" >Edit</a>

</dd>
</div>
<div id="bfgs_old,proc(Tensor[U]),Tensor[U],U,U,bool,proc(Tensor[U])">
<dt><pre><span class="Keyword">proc</span> <a href="#bfgs_old%2Cproc%28Tensor%5BU%5D%29%2CTensor%5BU%5D%2CU%2CU%2Cbool%2Cproc%28Tensor%5BU%5D%29"><span class="Identifier">bfgs_old</span></a><span class="Other">[</span><span class="Identifier">U</span><span class="Other">;</span> <span class="Identifier">T</span><span class="Other">:</span> <span class="Keyword">not</span> <span class="Identifier">Tensor</span><span class="Other">]</span><span class="Other">(</span><span class="Identifier">f</span><span class="Other">:</span> <span class="Keyword">proc</span> <span class="Other">(</span><span class="Identifier">x</span><span class="Other">:</span> <span class="Identifier">Tensor</span><span class="Other">[</span><span class="Identifier">U</span><span class="Other">]</span><span class="Other">)</span><span class="Other">:</span> <span class="Identifier">T</span><span class="Other">;</span> <span class="Identifier">x0</span><span class="Other">:</span> <span class="Identifier">Tensor</span><span class="Other">[</span><span class="Identifier">U</span><span class="Other">]</span><span class="Other">;</span>
                                <span class="Identifier">alpha</span><span class="Other">:</span> <span class="Identifier">U</span> <span class="Other">=</span> <span class="Identifier">U</span><span class="Other">(</span><span class="DecNumber">1</span><span class="Other">)</span><span class="Other">;</span> <span class="Identifier">tol</span><span class="Other">:</span> <span class="Identifier">U</span> <span class="Other">=</span> <span class="Identifier">U</span><span class="Other">(</span><span class="FloatNumber">0.000001</span><span class="Other">)</span><span class="Other">;</span>
                                <span class="Identifier">fastMode</span><span class="Other">:</span> <span class="Identifier">bool</span> <span class="Other">=</span> <span class="Identifier">false</span><span class="Other">;</span> <span class="Identifier">analyticGradient</span><span class="Other">:</span> <span class="Keyword">proc</span> <span class="Other">(</span>
    <span class="Identifier">x</span><span class="Other">:</span> <span class="Identifier">Tensor</span><span class="Other">[</span><span class="Identifier">U</span><span class="Other">]</span><span class="Other">)</span><span class="Other">:</span> <span class="Identifier">Tensor</span><span class="Other">[</span><span class="Identifier">T</span><span class="Other">]</span> <span class="Other">=</span> <span class="Keyword">nil</span><span class="Other">)</span><span class="Other">:</span> <span class="Identifier">Tensor</span><span class="Other">[</span><span class="Identifier">U</span><span class="Other">]</span></pre></dt>
<dd>


&nbsp;&nbsp;<a
href="https://github.com/SciNim/numericalnim/tree/master/src/numericalnim/optimize.nim#L414"
class="link-seesrc" target="_blank">Source</a>
&nbsp;&nbsp;<a href="https://github.com/SciNim/numericalnim/edit/devel/src/numericalnim/optimize.nim#L414" class="link-seesrc" target="_blank" >Edit</a>

</dd>
</div>
<div id="bfgsOptions,U,U,bool,int,LineSearchCriterion">
<dt><pre><span class="Keyword">proc</span> <a href="#bfgsOptions%2CU%2CU%2Cbool%2Cint%2CLineSearchCriterion"><span class="Identifier">bfgsOptions</span></a><span class="Other">[</span><span class="Identifier">U</span><span class="Other">]</span><span class="Other">(</span><span class="Identifier">tol</span><span class="Other">:</span> <span class="Identifier">U</span> <span class="Other">=</span> <span class="Identifier">U</span><span class="Other">(</span><span class="FloatNumber">0.000001</span><span class="Other">)</span><span class="Other">;</span> <span class="Identifier">alpha</span><span class="Other">:</span> <span class="Identifier">U</span> <span class="Other">=</span> <span class="Identifier">U</span><span class="Other">(</span><span class="DecNumber">1</span><span class="Other">)</span><span class="Other">;</span>
                    <span class="Identifier">fastMode</span><span class="Other">:</span> <span class="Identifier">bool</span> <span class="Other">=</span> <span class="Identifier">false</span><span class="Other">;</span> <span class="Identifier">maxIterations</span><span class="Other">:</span> <span class="Identifier">int</span> <span class="Other">=</span> <span class="DecNumber">10000</span><span class="Other">;</span>
                    <span class="Identifier">lineSearchCriterion</span><span class="Other">:</span> <a href="optimize.html#LineSearchCriterion"><span class="Identifier">LineSearchCriterion</span></a> <span class="Other">=</span> <span class="Identifier">NoLineSearch</span><span class="Other">)</span><span class="Other">:</span> <a href="optimize.html#OptimOptions"><span class="Identifier">OptimOptions</span></a><span class="Other">[</span>
    <span class="Identifier">U</span><span class="Other">,</span> <a href="optimize.html#StandardOptions"><span class="Identifier">StandardOptions</span></a><span class="Other">]</span></pre></dt>
<dd>

Returns a BFGS OptimOptions<ul class="simple"><li>tol: The tolerance used. This is the criteria for convergence: <tt class="docutils literal"><span class="pre"><span class="Identifier">gradNorm</span> <span class="Operator">&lt;</span> <span class="Identifier">tol</span><span class="Operator">*</span><span class="Punctuation">(</span><span class="DecNumber">1</span> <span class="Operator">+</span> <span class="Identifier">fNorm</span><span class="Punctuation">)</span></span></tt>.</li>
<li>alpha: The step size.</li>
<li>fastMode: If true, a faster first order accurate finite difference approximation of the derivative will be used. Else a more accurate but slowe second order finite difference scheme will be used.</li>
<li>maxIteration: The maximum number of iteration before returning if convergence haven't been reached.</li>
<li>lineSearchCriterion: Which line search method to use.</li>
</ul>

&nbsp;&nbsp;<a
href="https://github.com/SciNim/numericalnim/tree/master/src/numericalnim/optimize.nim#L245"
class="link-seesrc" target="_blank">Source</a>
&nbsp;&nbsp;<a href="https://github.com/SciNim/numericalnim/edit/devel/src/numericalnim/optimize.nim#L245" class="link-seesrc" target="_blank" >Edit</a>

</dd>
</div>
<div id="conjugate_gradient,Tensor[T],Tensor[T],Tensor[T],float64">
<dt><pre><span class="Keyword">proc</span> <a href="#conjugate_gradient%2CTensor%5BT%5D%2CTensor%5BT%5D%2CTensor%5BT%5D%2Cfloat64"><span class="Identifier">conjugate_gradient</span></a><span class="Other">[</span><span class="Identifier">T</span><span class="Other">]</span><span class="Other">(</span><span class="Identifier">A</span><span class="Other">,</span> <span class="Identifier">b</span><span class="Other">,</span> <span class="Identifier">x_0</span><span class="Other">:</span> <span class="Identifier">Tensor</span><span class="Other">[</span><span class="Identifier">T</span><span class="Other">]</span><span class="Other">;</span> <span class="Identifier">tolerance</span><span class="Other">:</span> <span class="Identifier">float64</span><span class="Other">)</span><span class="Other">:</span> <span class="Identifier">Tensor</span><span class="Other">[</span><span class="Identifier">T</span><span class="Other">]</span></pre></dt>
<dd>

Conjugate Gradient method. Given a Symmetric and Positive-Definite matrix A, solve the linear system Ax = b Symmetric Matrix: Square matrix that is equal to its transpose, transpose(A) == A Positive Definite: Square matrix such that transpose(x)Ax &gt; 0 for all x in R^n<dl class="docutils"><dt>Input:</dt>
<dd><ul class="simple"><li>A: NxN square matrix</li>
<li>b: vector on the right side of Ax=b</li>
<li>x_0: Initial guess vector</li>
</ul>
</dd>
<dt>Returns:</dt>
<dd><ul class="simple"><li>Tensor.</li>
</ul>
</dd>
</dl>

&nbsp;&nbsp;<a
href="https://github.com/SciNim/numericalnim/tree/master/src/numericalnim/optimize.nim#L96"
class="link-seesrc" target="_blank">Source</a>
&nbsp;&nbsp;<a href="https://github.com/SciNim/numericalnim/edit/devel/src/numericalnim/optimize.nim#L96" class="link-seesrc" target="_blank" >Edit</a>

</dd>
</div>
<div id="lbfgs,proc(Tensor[U]),Tensor[U],OptimOptions[U,LBFGSOptions[U]],proc(Tensor[U])">
<dt><pre><span class="Keyword">proc</span> <a href="#lbfgs%2Cproc%28Tensor%5BU%5D%29%2CTensor%5BU%5D%2COptimOptions%5BU%2CLBFGSOptions%5BU%5D%5D%2Cproc%28Tensor%5BU%5D%29"><span class="Identifier">lbfgs</span></a><span class="Other">[</span><span class="Identifier">U</span><span class="Other">;</span> <span class="Identifier">T</span><span class="Other">:</span> <span class="Keyword">not</span> <span class="Identifier">Tensor</span><span class="Other">]</span><span class="Other">(</span><span class="Identifier">f</span><span class="Other">:</span> <span class="Keyword">proc</span> <span class="Other">(</span><span class="Identifier">x</span><span class="Other">:</span> <span class="Identifier">Tensor</span><span class="Other">[</span><span class="Identifier">U</span><span class="Other">]</span><span class="Other">)</span><span class="Other">:</span> <span class="Identifier">T</span><span class="Other">;</span> <span class="Identifier">x0</span><span class="Other">:</span> <span class="Identifier">Tensor</span><span class="Other">[</span><span class="Identifier">U</span><span class="Other">]</span><span class="Other">;</span> <span class="Identifier">options</span><span class="Other">:</span> <a href="optimize.html#OptimOptions"><span class="Identifier">OptimOptions</span></a><span class="Other">[</span>
    <span class="Identifier">U</span><span class="Other">,</span> <a href="optimize.html#LBFGSOptions"><span class="Identifier">LBFGSOptions</span></a><span class="Other">[</span><span class="Identifier">U</span><span class="Other">]</span><span class="Other">]</span> <span class="Other">=</span> <span class="Identifier">lbfgsOptions</span><span class="Other">[</span><span class="Identifier">U</span><span class="Other">]</span><span class="Other">(</span><span class="Other">)</span><span class="Other">;</span> <span class="Identifier">analyticGradient</span><span class="Other">:</span> <span class="Keyword">proc</span> <span class="Other">(</span>
    <span class="Identifier">x</span><span class="Other">:</span> <span class="Identifier">Tensor</span><span class="Other">[</span><span class="Identifier">U</span><span class="Other">]</span><span class="Other">)</span><span class="Other">:</span> <span class="Identifier">Tensor</span><span class="Other">[</span><span class="Identifier">T</span><span class="Other">]</span> <span class="Other">=</span> <span class="Keyword">nil</span><span class="Other">)</span><span class="Other">:</span> <span class="Identifier">Tensor</span><span class="Other">[</span><span class="Identifier">U</span><span class="Other">]</span></pre></dt>
<dd>

<p>LBFGS (Limited-memory  Broyden–Fletcher–Goldfarb–Shanno) method for optimization.</p>
<p>Inputs:</p>
<ul class="simple"><li>f: The function to optimize. It should take as input a 1D Tensor of the input variables and return a scalar.</li>
<li>options: Options object (see <tt class="docutils literal"><span class="pre"><span class="Identifier">lbfgsOptions</span></span></tt> for constructing one)</li>
<li>analyticGradient: The analytic gradient of <tt class="docutils literal"><span class="pre"><span class="Identifier">f</span></span></tt> taking in and returning a 1D Tensor. If not provided, a finite difference approximation will be performed instead.</li>
</ul>
<p>Returns:</p>
<ul class="simple"><li>The final solution for the parameters. Either because a (local) minimum was found or because the maximum number of iterations was reached.</li>
</ul>

&nbsp;&nbsp;<a
href="https://github.com/SciNim/numericalnim/tree/master/src/numericalnim/optimize.nim#L547"
class="link-seesrc" target="_blank">Source</a>
&nbsp;&nbsp;<a href="https://github.com/SciNim/numericalnim/edit/devel/src/numericalnim/optimize.nim#L547" class="link-seesrc" target="_blank" >Edit</a>

</dd>
</div>
<div id="lbfgsOptions,int,U,U,bool,int,LineSearchCriterion">
<dt><pre><span class="Keyword">proc</span> <a href="#lbfgsOptions%2Cint%2CU%2CU%2Cbool%2Cint%2CLineSearchCriterion"><span class="Identifier">lbfgsOptions</span></a><span class="Other">[</span><span class="Identifier">U</span><span class="Other">]</span><span class="Other">(</span><span class="Identifier">savedIterations</span><span class="Other">:</span> <span class="Identifier">int</span> <span class="Other">=</span> <span class="DecNumber">10</span><span class="Other">;</span> <span class="Identifier">tol</span><span class="Other">:</span> <span class="Identifier">U</span> <span class="Other">=</span> <span class="Identifier">U</span><span class="Other">(</span><span class="FloatNumber">0.000001</span><span class="Other">)</span><span class="Other">;</span>
                     <span class="Identifier">alpha</span><span class="Other">:</span> <span class="Identifier">U</span> <span class="Other">=</span> <span class="Identifier">U</span><span class="Other">(</span><span class="DecNumber">1</span><span class="Other">)</span><span class="Other">;</span> <span class="Identifier">fastMode</span><span class="Other">:</span> <span class="Identifier">bool</span> <span class="Other">=</span> <span class="Identifier">false</span><span class="Other">;</span>
                     <span class="Identifier">maxIterations</span><span class="Other">:</span> <span class="Identifier">int</span> <span class="Other">=</span> <span class="DecNumber">10000</span><span class="Other">;</span>
                     <span class="Identifier">lineSearchCriterion</span><span class="Other">:</span> <a href="optimize.html#LineSearchCriterion"><span class="Identifier">LineSearchCriterion</span></a> <span class="Other">=</span> <span class="Identifier">NoLineSearch</span><span class="Other">)</span><span class="Other">:</span> <a href="optimize.html#OptimOptions"><span class="Identifier">OptimOptions</span></a><span class="Other">[</span>
    <span class="Identifier">U</span><span class="Other">,</span> <a href="optimize.html#LBFGSOptions"><span class="Identifier">LBFGSOptions</span></a><span class="Other">[</span><span class="Identifier">U</span><span class="Other">]</span><span class="Other">]</span></pre></dt>
<dd>

Returns a LBFGS OptimOptions<ul class="simple"><li>tol: The tolerance used. This is the criteria for convergence: <tt class="docutils literal"><span class="pre"><span class="Identifier">gradNorm</span> <span class="Operator">&lt;</span> <span class="Identifier">tol</span><span class="Operator">*</span><span class="Punctuation">(</span><span class="DecNumber">1</span> <span class="Operator">+</span> <span class="Identifier">fNorm</span><span class="Punctuation">)</span></span></tt>.</li>
<li>alpha: The step size.</li>
<li>fastMode: If true, a faster first order accurate finite difference approximation of the derivative will be used. Else a more accurate but slowe second order finite difference scheme will be used.</li>
<li>maxIteration: The maximum number of iteration before returning if convergence haven't been reached.</li>
<li>lineSearchCriterion: Which line search method to use.</li>
<li>savedIterations: Number of past iterations to save. The higher the value, the better but slower steps.</li>
</ul>

&nbsp;&nbsp;<a
href="https://github.com/SciNim/numericalnim/tree/master/src/numericalnim/optimize.nim#L259"
class="link-seesrc" target="_blank">Source</a>
&nbsp;&nbsp;<a href="https://github.com/SciNim/numericalnim/edit/devel/src/numericalnim/optimize.nim#L259" class="link-seesrc" target="_blank" >Edit</a>

</dd>
</div>
<div id="levmarq,proc(Tensor[U],U),Tensor[U],Tensor[U],Tensor[T: not Tensor],OptimOptions[U,LevMarqOptions[U]],Tensor[T: not Tensor]">
<dt><pre><span class="Keyword">proc</span> <a href="#levmarq%2Cproc%28Tensor%5BU%5D%2CU%29%2CTensor%5BU%5D%2CTensor%5BU%5D%2CTensor%5BT%3A%20not%20Tensor%5D%2COptimOptions%5BU%2CLevMarqOptions%5BU%5D%5D%2CTensor%5BT%3A%20not%20Tensor%5D"><span class="Identifier">levmarq</span></a><span class="Other">[</span><span class="Identifier">U</span><span class="Other">;</span> <span class="Identifier">T</span><span class="Other">:</span> <span class="Keyword">not</span> <span class="Identifier">Tensor</span><span class="Other">]</span><span class="Other">(</span><span class="Identifier">f</span><span class="Other">:</span> <span class="Keyword">proc</span> <span class="Other">(</span><span class="Identifier">params</span><span class="Other">:</span> <span class="Identifier">Tensor</span><span class="Other">[</span><span class="Identifier">U</span><span class="Other">]</span><span class="Other">;</span> <span class="Identifier">x</span><span class="Other">:</span> <span class="Identifier">U</span><span class="Other">)</span><span class="Other">:</span> <span class="Identifier">T</span><span class="Other">;</span>
                               <span class="Identifier">params0</span><span class="Other">:</span> <span class="Identifier">Tensor</span><span class="Other">[</span><span class="Identifier">U</span><span class="Other">]</span><span class="Other">;</span> <span class="Identifier">xData</span><span class="Other">:</span> <span class="Identifier">Tensor</span><span class="Other">[</span><span class="Identifier">U</span><span class="Other">]</span><span class="Other">;</span>
                               <span class="Identifier">yData</span><span class="Other">:</span> <span class="Identifier">Tensor</span><span class="Other">[</span><span class="Identifier">T</span><span class="Other">]</span><span class="Other">;</span> <span class="Identifier">options</span><span class="Other">:</span> <a href="optimize.html#OptimOptions"><span class="Identifier">OptimOptions</span></a><span class="Other">[</span><span class="Identifier">U</span><span class="Other">,</span>
    <a href="optimize.html#LevMarqOptions"><span class="Identifier">LevMarqOptions</span></a><span class="Other">[</span><span class="Identifier">U</span><span class="Other">]</span><span class="Other">]</span> <span class="Other">=</span> <span class="Identifier">levmarqOptions</span><span class="Other">[</span><span class="Identifier">U</span><span class="Other">]</span><span class="Other">(</span><span class="Other">)</span><span class="Other">;</span>
                               <span class="Identifier">yError</span><span class="Other">:</span> <span class="Identifier">Tensor</span><span class="Other">[</span><span class="Identifier">T</span><span class="Other">]</span> <span class="Other">=</span> <span class="Identifier">ones_like</span><span class="Other">(</span><span class="Identifier">yData</span><span class="Other">)</span><span class="Other">)</span><span class="Other">:</span> <span class="Identifier">Tensor</span><span class="Other">[</span><span class="Identifier">U</span><span class="Other">]</span></pre></dt>
<dd>

<p>Levenberg-Marquardt for non-linear least square solving. Basically it fits parameters of a function to data samples.</p>
<p>Input:</p>
<ul class="simple"><li><dl class="docutils"><dt>f: The function you want to fit the data to. The first argument should be a 1D Tensor with the values of the parameters</dt>
<dd>and the second argument is the value if the independent variable to evaluate the function at.</dd>
</dl>
</li>
<li>params0: The starting guess for the parameter values as a 1D Tensor.</li>
<li>yData: The measured values of the dependent variable as 1D Tensor.</li>
<li>xData: The values of the independent variable as 1D Tensor.</li>
<li>options: Object with all the options like <tt class="docutils literal"><span class="pre"><span class="Identifier">tol</span></span></tt> and <tt class="docutils literal"><span class="pre"><span class="Identifier">lambda0</span></span></tt>. (see <tt class="docutils literal"><span class="pre"><span class="Identifier">levmarqOptions</span></span></tt>)</li>
<li>yError: The uncertainties of the <tt class="docutils literal"><span class="pre"><span class="Identifier">yData</span></span></tt> as 1D Tensor. Ideally these should be the 1σ standard deviation.</li>
</ul>
<p>Returns:</p>
<ul class="simple"><li>The final solution for the parameters. Either because a (local) minimum was found or because the maximum number of iterations was reached.</li>
</ul>

&nbsp;&nbsp;<a
href="https://github.com/SciNim/numericalnim/tree/master/src/numericalnim/optimize.nim#L610"
class="link-seesrc" target="_blank">Source</a>
&nbsp;&nbsp;<a href="https://github.com/SciNim/numericalnim/edit/devel/src/numericalnim/optimize.nim#L610" class="link-seesrc" target="_blank" >Edit</a>

</dd>
</div>
<div id="levmarqOptions,U,U,U,bool,int,LineSearchCriterion">
<dt><pre><span class="Keyword">proc</span> <a href="#levmarqOptions%2CU%2CU%2CU%2Cbool%2Cint%2CLineSearchCriterion"><span class="Identifier">levmarqOptions</span></a><span class="Other">[</span><span class="Identifier">U</span><span class="Other">]</span><span class="Other">(</span><span class="Identifier">lambda0</span><span class="Other">:</span> <span class="Identifier">U</span> <span class="Other">=</span> <span class="Identifier">U</span><span class="Other">(</span><span class="DecNumber">1</span><span class="Other">)</span><span class="Other">;</span> <span class="Identifier">tol</span><span class="Other">:</span> <span class="Identifier">U</span> <span class="Other">=</span> <span class="Identifier">U</span><span class="Other">(</span><span class="FloatNumber">0.000001</span><span class="Other">)</span><span class="Other">;</span> <span class="Identifier">alpha</span><span class="Other">:</span> <span class="Identifier">U</span> <span class="Other">=</span> <span class="Identifier">U</span><span class="Other">(</span><span class="DecNumber">1</span><span class="Other">)</span><span class="Other">;</span>
                       <span class="Identifier">fastMode</span><span class="Other">:</span> <span class="Identifier">bool</span> <span class="Other">=</span> <span class="Identifier">false</span><span class="Other">;</span> <span class="Identifier">maxIterations</span><span class="Other">:</span> <span class="Identifier">int</span> <span class="Other">=</span> <span class="DecNumber">10000</span><span class="Other">;</span>
                       <span class="Identifier">lineSearchCriterion</span><span class="Other">:</span> <a href="optimize.html#LineSearchCriterion"><span class="Identifier">LineSearchCriterion</span></a> <span class="Other">=</span> <span class="Identifier">NoLineSearch</span><span class="Other">)</span><span class="Other">:</span> <a href="optimize.html#OptimOptions"><span class="Identifier">OptimOptions</span></a><span class="Other">[</span>
    <span class="Identifier">U</span><span class="Other">,</span> <a href="optimize.html#LevMarqOptions"><span class="Identifier">LevMarqOptions</span></a><span class="Other">[</span><span class="Identifier">U</span><span class="Other">]</span><span class="Other">]</span></pre></dt>
<dd>

Returns a levmarq OptimOptions<ul class="simple"><li>tol: The tolerance used. This is the criteria for convergence: <tt class="docutils literal"><span class="pre"><span class="Identifier">gradNorm</span> <span class="Operator">&lt;</span> <span class="Identifier">tol</span><span class="Operator">*</span><span class="Punctuation">(</span><span class="DecNumber">1</span> <span class="Operator">+</span> <span class="Identifier">fNorm</span><span class="Punctuation">)</span></span></tt>.</li>
<li>alpha: The step size.</li>
<li>fastMode: If true, a faster first order accurate finite difference approximation of the derivative will be used. Else a more accurate but slowe second order finite difference scheme will be used.</li>
<li>maxIteration: The maximum number of iteration before returning if convergence haven't been reached.</li>
<li>lineSearchCriterion: Which line search method to use.</li>
<li>lambda0: Starting value of dampening parameter</li>
</ul>

&nbsp;&nbsp;<a
href="https://github.com/SciNim/numericalnim/tree/master/src/numericalnim/optimize.nim#L275"
class="link-seesrc" target="_blank">Source</a>
&nbsp;&nbsp;<a href="https://github.com/SciNim/numericalnim/edit/devel/src/numericalnim/optimize.nim#L275" class="link-seesrc" target="_blank" >Edit</a>

</dd>
</div>
<div id="line_search,U,Tensor[T],Tensor[U],proc(Tensor[U]),LineSearchCriterion,bool">
<dt><pre><span class="Keyword">proc</span> <a href="#line_search%2CU%2CTensor%5BT%5D%2CTensor%5BU%5D%2Cproc%28Tensor%5BU%5D%29%2CLineSearchCriterion%2Cbool"><span class="Identifier">line_search</span></a><span class="Other">[</span><span class="Identifier">U</span><span class="Other">,</span> <span class="Identifier">T</span><span class="Other">]</span><span class="Other">(</span><span class="Identifier">alpha</span><span class="Other">:</span> <span class="Keyword">var</span> <span class="Identifier">U</span><span class="Other">;</span> <span class="Identifier">p</span><span class="Other">:</span> <span class="Identifier">Tensor</span><span class="Other">[</span><span class="Identifier">T</span><span class="Other">]</span><span class="Other">;</span> <span class="Identifier">x0</span><span class="Other">:</span> <span class="Identifier">Tensor</span><span class="Other">[</span><span class="Identifier">U</span><span class="Other">]</span><span class="Other">;</span>
                       <span class="Identifier">f</span><span class="Other">:</span> <span class="Keyword">proc</span> <span class="Other">(</span><span class="Identifier">x</span><span class="Other">:</span> <span class="Identifier">Tensor</span><span class="Other">[</span><span class="Identifier">U</span><span class="Other">]</span><span class="Other">)</span><span class="Other">:</span> <span class="Identifier">T</span><span class="Other">;</span>
                       <span class="Identifier">criterion</span><span class="Other">:</span> <a href="optimize.html#LineSearchCriterion"><span class="Identifier">LineSearchCriterion</span></a><span class="Other">;</span> <span class="Identifier">fastMode</span><span class="Other">:</span> <span class="Identifier">bool</span> <span class="Other">=</span> <span class="Identifier">false</span><span class="Other">)</span></pre></dt>
<dd>


&nbsp;&nbsp;<a
href="https://github.com/SciNim/numericalnim/tree/master/src/numericalnim/optimize.nim#L304"
class="link-seesrc" target="_blank">Source</a>
&nbsp;&nbsp;<a href="https://github.com/SciNim/numericalnim/edit/devel/src/numericalnim/optimize.nim#L304" class="link-seesrc" target="_blank" >Edit</a>

</dd>
</div>
<div id="newton,proc(Tensor[U]),Tensor[U],OptimOptions[U,StandardOptions],proc(Tensor[U])">
<dt><pre><span class="Keyword">proc</span> <a href="#newton%2Cproc%28Tensor%5BU%5D%29%2CTensor%5BU%5D%2COptimOptions%5BU%2CStandardOptions%5D%2Cproc%28Tensor%5BU%5D%29"><span class="Identifier">newton</span></a><span class="Other">[</span><span class="Identifier">U</span><span class="Other">;</span> <span class="Identifier">T</span><span class="Other">:</span> <span class="Keyword">not</span> <span class="Identifier">Tensor</span><span class="Other">]</span><span class="Other">(</span><span class="Identifier">f</span><span class="Other">:</span> <span class="Keyword">proc</span> <span class="Other">(</span><span class="Identifier">x</span><span class="Other">:</span> <span class="Identifier">Tensor</span><span class="Other">[</span><span class="Identifier">U</span><span class="Other">]</span><span class="Other">)</span><span class="Other">:</span> <span class="Identifier">T</span><span class="Other">;</span> <span class="Identifier">x0</span><span class="Other">:</span> <span class="Identifier">Tensor</span><span class="Other">[</span><span class="Identifier">U</span><span class="Other">]</span><span class="Other">;</span> <span class="Identifier">options</span><span class="Other">:</span> <a href="optimize.html#OptimOptions"><span class="Identifier">OptimOptions</span></a><span class="Other">[</span>
    <span class="Identifier">U</span><span class="Other">,</span> <a href="optimize.html#StandardOptions"><span class="Identifier">StandardOptions</span></a><span class="Other">]</span> <span class="Other">=</span> <span class="Identifier">newtonOptions</span><span class="Other">[</span><span class="Identifier">U</span><span class="Other">]</span><span class="Other">(</span><span class="Other">)</span><span class="Other">;</span> <span class="Identifier">analyticGradient</span><span class="Other">:</span> <span class="Keyword">proc</span> <span class="Other">(</span>
    <span class="Identifier">x</span><span class="Other">:</span> <span class="Identifier">Tensor</span><span class="Other">[</span><span class="Identifier">U</span><span class="Other">]</span><span class="Other">)</span><span class="Other">:</span> <span class="Identifier">Tensor</span><span class="Other">[</span><span class="Identifier">T</span><span class="Other">]</span> <span class="Other">=</span> <span class="Keyword">nil</span><span class="Other">)</span><span class="Other">:</span> <span class="Identifier">Tensor</span><span class="Other">[</span><span class="Identifier">U</span><span class="Other">]</span></pre></dt>
<dd>

<p>Newton's method for optimization.</p>
<p>Inputs:</p>
<ul class="simple"><li>f: The function to optimize. It should take as input a 1D Tensor of the input variables and return a scalar.</li>
<li>options: Options object (see <tt class="docutils literal"><span class="pre"><span class="Identifier">newtonOptions</span></span></tt> for constructing one)</li>
<li>analyticGradient: The analytic gradient of <tt class="docutils literal"><span class="pre"><span class="Identifier">f</span></span></tt> taking in and returning a 1D Tensor. If not provided, a finite difference approximation will be performed instead.</li>
</ul>
<p>Returns:</p>
<ul class="simple"><li>The final solution for the parameters. Either because a (local) minimum was found or because the maximum number of iterations was reached.</li>
</ul>

&nbsp;&nbsp;<a
href="https://github.com/SciNim/numericalnim/tree/master/src/numericalnim/optimize.nim#L382"
class="link-seesrc" target="_blank">Source</a>
&nbsp;&nbsp;<a href="https://github.com/SciNim/numericalnim/edit/devel/src/numericalnim/optimize.nim#L382" class="link-seesrc" target="_blank" >Edit</a>

</dd>
</div>
<div id="newtonOptions,U,U,bool,int,LineSearchCriterion">
<dt><pre><span class="Keyword">proc</span> <a href="#newtonOptions%2CU%2CU%2Cbool%2Cint%2CLineSearchCriterion"><span class="Identifier">newtonOptions</span></a><span class="Other">[</span><span class="Identifier">U</span><span class="Other">]</span><span class="Other">(</span><span class="Identifier">tol</span><span class="Other">:</span> <span class="Identifier">U</span> <span class="Other">=</span> <span class="Identifier">U</span><span class="Other">(</span><span class="FloatNumber">0.000001</span><span class="Other">)</span><span class="Other">;</span> <span class="Identifier">alpha</span><span class="Other">:</span> <span class="Identifier">U</span> <span class="Other">=</span> <span class="Identifier">U</span><span class="Other">(</span><span class="DecNumber">1</span><span class="Other">)</span><span class="Other">;</span>
                      <span class="Identifier">fastMode</span><span class="Other">:</span> <span class="Identifier">bool</span> <span class="Other">=</span> <span class="Identifier">false</span><span class="Other">;</span> <span class="Identifier">maxIterations</span><span class="Other">:</span> <span class="Identifier">int</span> <span class="Other">=</span> <span class="DecNumber">10000</span><span class="Other">;</span>
                      <span class="Identifier">lineSearchCriterion</span><span class="Other">:</span> <a href="optimize.html#LineSearchCriterion"><span class="Identifier">LineSearchCriterion</span></a> <span class="Other">=</span> <span class="Identifier">NoLineSearch</span><span class="Other">)</span><span class="Other">:</span> <a href="optimize.html#OptimOptions"><span class="Identifier">OptimOptions</span></a><span class="Other">[</span>
    <span class="Identifier">U</span><span class="Other">,</span> <a href="optimize.html#StandardOptions"><span class="Identifier">StandardOptions</span></a><span class="Other">]</span></pre></dt>
<dd>

Returns a Newton OptimOptions<ul class="simple"><li>tol: The tolerance used. This is the criteria for convergence: <tt class="docutils literal"><span class="pre"><span class="Identifier">gradNorm</span> <span class="Operator">&lt;</span> <span class="Identifier">tol</span><span class="Operator">*</span><span class="Punctuation">(</span><span class="DecNumber">1</span> <span class="Operator">+</span> <span class="Identifier">fNorm</span><span class="Punctuation">)</span></span></tt>.</li>
<li>alpha: The step size.</li>
<li>fastMode: If true, a faster first order accurate finite difference approximation of the derivative will be used. Else a more accurate but slowe second order finite difference scheme will be used.</li>
<li>maxIteration: The maximum number of iteration before returning if convergence haven't been reached.</li>
<li>lineSearchCriterion: Which line search method to use.</li>
</ul>

&nbsp;&nbsp;<a
href="https://github.com/SciNim/numericalnim/tree/master/src/numericalnim/optimize.nim#L231"
class="link-seesrc" target="_blank">Source</a>
&nbsp;&nbsp;<a href="https://github.com/SciNim/numericalnim/edit/devel/src/numericalnim/optimize.nim#L231" class="link-seesrc" target="_blank" >Edit</a>

</dd>
</div>
<div id="newtons,proc(float64),proc(float64),float64,float64,Natural">
<dt><pre><span class="Keyword">proc</span> <a href="#newtons%2Cproc%28float64%29%2Cproc%28float64%29%2Cfloat64%2Cfloat64%2CNatural"><span class="Identifier">newtons</span></a><span class="Other">(</span><span class="Identifier">f</span><span class="Other">:</span> <span class="Keyword">proc</span> <span class="Other">(</span><span class="Identifier">x</span><span class="Other">:</span> <span class="Identifier">float64</span><span class="Other">)</span><span class="Other">:</span> <span class="Identifier">float64</span><span class="Other">;</span> <span class="Identifier">deriv</span><span class="Other">:</span> <span class="Keyword">proc</span> <span class="Other">(</span><span class="Identifier">x</span><span class="Other">:</span> <span class="Identifier">float64</span><span class="Other">)</span><span class="Other">:</span> <span class="Identifier">float64</span><span class="Other">;</span>
             <span class="Identifier">start</span><span class="Other">:</span> <span class="Identifier">float64</span><span class="Other">;</span> <span class="Identifier">precision</span><span class="Other">:</span> <span class="Identifier">float64</span> <span class="Other">=</span> <span class="FloatNumber">0.00001</span><span class="Other">;</span>
             <span class="Identifier">max_iters</span><span class="Other">:</span> <span class="Identifier">Natural</span> <span class="Other">=</span> <span class="DecNumber">1000</span><span class="Other">)</span><span class="Other">:</span> <span class="Identifier">float64</span> {.<span><span class="Other pragmadots">...</span></span><span class="pragmawrap"><span class="Identifier">raises</span><span class="Other">:</span> <span class="Other">[</span><span class="Identifier">ArithmeticError</span><span class="Other">]</span><span class="Other">,</span>
    </span><span class="Identifier">effectsOf</span><span class="Other">:</span> <span class="Other">[</span><span class="Identifier">f</span><span class="Other">,</span> <span class="Identifier">deriv</span><span class="Other">]</span><span class="Other">,</span> <span><span class="Other pragmadots">...</span></span><span class="pragmawrap"><span class="Identifier">tags</span><span class="Other">:</span> <span class="Other">[</span><span class="Other">]</span></span>.}</pre></dt>
<dd>

Newton-Raphson implementation for 1-dimensional functions
&nbsp;&nbsp;<a
href="https://github.com/SciNim/numericalnim/tree/master/src/numericalnim/optimize.nim#L135"
class="link-seesrc" target="_blank">Source</a>
&nbsp;&nbsp;<a href="https://github.com/SciNim/numericalnim/edit/devel/src/numericalnim/optimize.nim#L135" class="link-seesrc" target="_blank" >Edit</a>

</dd>
</div>
<div id="optimOptions,U,U,bool,int,LineSearchCriterion">
<dt><pre><span class="Keyword">proc</span> <a href="#optimOptions%2CU%2CU%2Cbool%2Cint%2CLineSearchCriterion"><span class="Identifier">optimOptions</span></a><span class="Other">[</span><span class="Identifier">U</span><span class="Other">]</span><span class="Other">(</span><span class="Identifier">tol</span><span class="Other">:</span> <span class="Identifier">U</span> <span class="Other">=</span> <span class="Identifier">U</span><span class="Other">(</span><span class="FloatNumber">0.000001</span><span class="Other">)</span><span class="Other">;</span> <span class="Identifier">alpha</span><span class="Other">:</span> <span class="Identifier">U</span> <span class="Other">=</span> <span class="Identifier">U</span><span class="Other">(</span><span class="DecNumber">1</span><span class="Other">)</span><span class="Other">;</span>
                     <span class="Identifier">fastMode</span><span class="Other">:</span> <span class="Identifier">bool</span> <span class="Other">=</span> <span class="Identifier">false</span><span class="Other">;</span> <span class="Identifier">maxIterations</span><span class="Other">:</span> <span class="Identifier">int</span> <span class="Other">=</span> <span class="DecNumber">10000</span><span class="Other">;</span>
                     <span class="Identifier">lineSearchCriterion</span><span class="Other">:</span> <a href="optimize.html#LineSearchCriterion"><span class="Identifier">LineSearchCriterion</span></a> <span class="Other">=</span> <span class="Identifier">NoLineSearch</span><span class="Other">)</span><span class="Other">:</span> <a href="optimize.html#OptimOptions"><span class="Identifier">OptimOptions</span></a><span class="Other">[</span>
    <span class="Identifier">U</span><span class="Other">,</span> <a href="optimize.html#StandardOptions"><span class="Identifier">StandardOptions</span></a><span class="Other">]</span></pre></dt>
<dd>

Returns a vanilla OptimOptions<ul class="simple"><li>tol: The tolerance used. This is the criteria for convergence: <tt class="docutils literal"><span class="pre"><span class="Identifier">gradNorm</span> <span class="Operator">&lt;</span> <span class="Identifier">tol</span><span class="Operator">*</span><span class="Punctuation">(</span><span class="DecNumber">1</span> <span class="Operator">+</span> <span class="Identifier">fNorm</span><span class="Punctuation">)</span></span></tt>.</li>
<li>alpha: The step size.</li>
<li>fastMode: If true, a faster first order accurate finite difference approximation of the derivative will be used. Else a more accurate but slowe second order finite difference scheme will be used.</li>
<li>maxIteration: The maximum number of iteration before returning if convergence haven't been reached.</li>
<li>lineSearchCriterion: Which line search method to use.</li>
</ul>

&nbsp;&nbsp;<a
href="https://github.com/SciNim/numericalnim/tree/master/src/numericalnim/optimize.nim#L203"
class="link-seesrc" target="_blank">Source</a>
&nbsp;&nbsp;<a href="https://github.com/SciNim/numericalnim/edit/devel/src/numericalnim/optimize.nim#L203" class="link-seesrc" target="_blank" >Edit</a>

</dd>
</div>
<div id="paramUncertainties,Tensor[U],proc(Tensor[U],U),Tensor[U],Tensor[T],Tensor[T]">
<dt><pre><span class="Keyword">proc</span> <a href="#paramUncertainties%2CTensor%5BU%5D%2Cproc%28Tensor%5BU%5D%2CU%29%2CTensor%5BU%5D%2CTensor%5BT%5D%2CTensor%5BT%5D"><span class="Identifier">paramUncertainties</span></a><span class="Other">[</span><span class="Identifier">U</span><span class="Other">;</span> <span class="Identifier">T</span><span class="Other">]</span><span class="Other">(</span><span class="Identifier">params</span><span class="Other">:</span> <span class="Identifier">Tensor</span><span class="Other">[</span><span class="Identifier">U</span><span class="Other">]</span><span class="Other">;</span>
                              <span class="Identifier">fitFunc</span><span class="Other">:</span> <span class="Keyword">proc</span> <span class="Other">(</span><span class="Identifier">params</span><span class="Other">:</span> <span class="Identifier">Tensor</span><span class="Other">[</span><span class="Identifier">U</span><span class="Other">]</span><span class="Other">;</span> <span class="Identifier">x</span><span class="Other">:</span> <span class="Identifier">U</span><span class="Other">)</span><span class="Other">:</span> <span class="Identifier">T</span><span class="Other">;</span>
                              <span class="Identifier">xData</span><span class="Other">:</span> <span class="Identifier">Tensor</span><span class="Other">[</span><span class="Identifier">U</span><span class="Other">]</span><span class="Other">;</span> <span class="Identifier">yData</span><span class="Other">:</span> <span class="Identifier">Tensor</span><span class="Other">[</span><span class="Identifier">T</span><span class="Other">]</span><span class="Other">;</span>
                              <span class="Identifier">yError</span><span class="Other">:</span> <span class="Identifier">Tensor</span><span class="Other">[</span><span class="Identifier">T</span><span class="Other">]</span><span class="Other">;</span> <span class="Identifier">returnFullCov</span> <span class="Other">=</span> <span class="Identifier">false</span><span class="Other">)</span><span class="Other">:</span> <span class="Identifier">Tensor</span><span class="Other">[</span>
    <span class="Identifier">T</span><span class="Other">]</span></pre></dt>
<dd>

<p>Returns the whole covariance matrix or only the diagonal elements for the parameters in <tt class="docutils literal"><span class="pre"><span class="Identifier">params</span></span></tt>.</p>
<p>Inputs:</p>
<ul class="simple"><li>params: The parameters in a 1D Tensor that the uncertainties are wanted for.</li>
<li>fitFunc: The function used for fitting the parameters. (see <tt class="docutils literal"><span class="pre"><span class="Identifier">levmarq</span></span></tt> for more)</li>
<li>xData: The values of the independent variable as 1D Tensor.</li>
<li>yData: The measured values of the dependent variable as 1D Tensor.</li>
<li>yError: The uncertainties of the <tt class="docutils literal"><span class="pre"><span class="Identifier">yData</span></span></tt> as 1D Tensor. Ideally these should be the 1σ standard deviation.</li>
<li>returnFullConv: If true, the full covariance matrix will be returned as a 2D Tensor, else only the diagonal elements will be returned as a 1D Tensor.</li>
</ul>
<p>Returns:</p>
<p>The uncertainties of the parameters in the form of a covariance matrix (or only the diagonal elements).</p>
<blockquote><p>Note: it is the covariance that is returned, so if you want the standard deviation you have to take the square root of it.</p></blockquote>

&nbsp;&nbsp;<a
href="https://github.com/SciNim/numericalnim/tree/master/src/numericalnim/optimize.nim#L681"
class="link-seesrc" target="_blank">Source</a>
&nbsp;&nbsp;<a href="https://github.com/SciNim/numericalnim/edit/devel/src/numericalnim/optimize.nim#L681" class="link-seesrc" target="_blank" >Edit</a>

</dd>
</div>
<div id="secant,proc(float64),array[,float64],float64,Natural">
<dt><pre><span class="Keyword">proc</span> <a href="#secant%2Cproc%28float64%29%2Carray%5B%2Cfloat64%5D%2Cfloat64%2CNatural"><span class="Identifier">secant</span></a><span class="Other">(</span><span class="Identifier">f</span><span class="Other">:</span> <span class="Keyword">proc</span> <span class="Other">(</span><span class="Identifier">x</span><span class="Other">:</span> <span class="Identifier">float64</span><span class="Other">)</span><span class="Other">:</span> <span class="Identifier">float64</span><span class="Other">;</span> <span class="Identifier">start</span><span class="Other">:</span> <span class="Identifier">array</span><span class="Other">[</span><span class="DecNumber">2</span><span class="Other">,</span> <span class="Identifier">float64</span><span class="Other">]</span><span class="Other">;</span>
            <span class="Identifier">precision</span><span class="Other">:</span> <span class="Identifier">float64</span> <span class="Other">=</span> <span class="FloatNumber">0.00001</span><span class="Other">;</span> <span class="Identifier">max_iters</span><span class="Other">:</span> <span class="Identifier">Natural</span> <span class="Other">=</span> <span class="DecNumber">1000</span><span class="Other">)</span><span class="Other">:</span> <span class="Identifier">float64</span> {.
    <span><span class="Other pragmadots">...</span></span><span class="pragmawrap"><span class="Identifier">raises</span><span class="Other">:</span> <span class="Other">[</span><span class="Other">]</span><span class="Other">,</span> <span class="Identifier">tags</span><span class="Other">:</span> <span class="Other">[</span><span class="Other">]</span></span>.}</pre></dt>
<dd>


&nbsp;&nbsp;<a
href="https://github.com/SciNim/numericalnim/tree/master/src/numericalnim/optimize.nim#L166"
class="link-seesrc" target="_blank">Source</a>
&nbsp;&nbsp;<a href="https://github.com/SciNim/numericalnim/edit/devel/src/numericalnim/optimize.nim#L166" class="link-seesrc" target="_blank" >Edit</a>

</dd>
</div>
<div id="steepest_descent,proc(float64),float64,float64,float64,Natural">
<dt><pre><span class="Keyword">proc</span> <a href="#steepest_descent%2Cproc%28float64%29%2Cfloat64%2Cfloat64%2Cfloat64%2CNatural"><span class="Identifier">steepest_descent</span></a><span class="Other">(</span><span class="Identifier">deriv</span><span class="Other">:</span> <span class="Keyword">proc</span> <span class="Other">(</span><span class="Identifier">x</span><span class="Other">:</span> <span class="Identifier">float64</span><span class="Other">)</span><span class="Other">:</span> <span class="Identifier">float64</span><span class="Other">;</span> <span class="Identifier">start</span><span class="Other">:</span> <span class="Identifier">float64</span><span class="Other">;</span>
                      <span class="Identifier">gamma</span><span class="Other">:</span> <span class="Identifier">float64</span> <span class="Other">=</span> <span class="FloatNumber">0.01</span><span class="Other">;</span> <span class="Identifier">precision</span><span class="Other">:</span> <span class="Identifier">float64</span> <span class="Other">=</span> <span class="FloatNumber">0.00001</span><span class="Other">;</span>
                      <span class="Identifier">max_iters</span><span class="Other">:</span> <span class="Identifier">Natural</span> <span class="Other">=</span> <span class="DecNumber">1000</span><span class="Other">)</span><span class="Other">:</span> <span class="Identifier">float64</span> {.<span class="Identifier">inline</span><span class="Other">,</span> <span><span class="Other pragmadots">...</span></span><span class="pragmawrap"><span class="Identifier">raises</span><span class="Other">:</span> <span class="Other">[</span><span class="Other">]</span><span class="Other">,</span>
    <span class="Identifier">tags</span><span class="Other">:</span> <span class="Other">[</span><span class="Other">]</span></span>.}</pre></dt>
<dd>

<p>Gradient descent optimization algorithm for finding local minimums of a function with derivative 'deriv'</p>
<p>Assuming that a multivariable function F is defined and differentiable near a minimum, F(x) decreases fastest when going in the direction negative to the gradient of F(a), similar to how water might traverse down a hill following the path of least resistance. can benefit from preconditioning if the condition number of the coefficient matrix is ill-conditioned</p>
<dl class="docutils"><dt>Input:</dt>
<dd><ul class="simple"><li>deriv: derivative of a multivariable function F</li>
<li>start: starting point near F's minimum</li>
<li>gamma: step size multiplier, used to control the step size between iterations</li>
<li>precision: numerical precision</li>
<li>max_iters: maximum iterations</li>
</ul>
</dd>
<dt>Returns:</dt>
<dd><ul class="simple"><li>float64.</li>
</ul>
</dd>
</dl>

&nbsp;&nbsp;<a
href="https://github.com/SciNim/numericalnim/tree/master/src/numericalnim/optimize.nim#L62"
class="link-seesrc" target="_blank">Source</a>
&nbsp;&nbsp;<a href="https://github.com/SciNim/numericalnim/edit/devel/src/numericalnim/optimize.nim#L62" class="link-seesrc" target="_blank" >Edit</a>

</dd>
</div>
<div id="steepestDescent,proc(Tensor[U]),Tensor[U],OptimOptions[U,StandardOptions],proc(Tensor[U])">
<dt><pre><span class="Keyword">proc</span> <a href="#steepestDescent%2Cproc%28Tensor%5BU%5D%29%2CTensor%5BU%5D%2COptimOptions%5BU%2CStandardOptions%5D%2Cproc%28Tensor%5BU%5D%29"><span class="Identifier">steepestDescent</span></a><span class="Other">[</span><span class="Identifier">U</span><span class="Other">;</span> <span class="Identifier">T</span><span class="Other">:</span> <span class="Keyword">not</span> <span class="Identifier">Tensor</span><span class="Other">]</span><span class="Other">(</span><span class="Identifier">f</span><span class="Other">:</span> <span class="Keyword">proc</span> <span class="Other">(</span><span class="Identifier">x</span><span class="Other">:</span> <span class="Identifier">Tensor</span><span class="Other">[</span><span class="Identifier">U</span><span class="Other">]</span><span class="Other">)</span><span class="Other">:</span> <span class="Identifier">T</span><span class="Other">;</span> <span class="Identifier">x0</span><span class="Other">:</span> <span class="Identifier">Tensor</span><span class="Other">[</span><span class="Identifier">U</span><span class="Other">]</span><span class="Other">;</span>
    <span class="Identifier">options</span><span class="Other">:</span> <a href="optimize.html#OptimOptions"><span class="Identifier">OptimOptions</span></a><span class="Other">[</span><span class="Identifier">U</span><span class="Other">,</span> <a href="optimize.html#StandardOptions"><span class="Identifier">StandardOptions</span></a><span class="Other">]</span> <span class="Other">=</span> <span class="Identifier">steepestDescentOptions</span><span class="Other">[</span><span class="Identifier">U</span><span class="Other">]</span><span class="Other">(</span><span class="Other">)</span><span class="Other">;</span>
    <span class="Identifier">analyticGradient</span><span class="Other">:</span> <span class="Keyword">proc</span> <span class="Other">(</span><span class="Identifier">x</span><span class="Other">:</span> <span class="Identifier">Tensor</span><span class="Other">[</span><span class="Identifier">U</span><span class="Other">]</span><span class="Other">)</span><span class="Other">:</span> <span class="Identifier">Tensor</span><span class="Other">[</span><span class="Identifier">T</span><span class="Other">]</span> <span class="Other">=</span> <span class="Keyword">nil</span><span class="Other">)</span><span class="Other">:</span> <span class="Identifier">Tensor</span><span class="Other">[</span><span class="Identifier">U</span><span class="Other">]</span></pre></dt>
<dd>

<p>Steepest descent method for optimization.</p>
<p>Inputs:</p>
<ul class="simple"><li>f: The function to optimize. It should take as input a 1D Tensor of the input variables and return a scalar.</li>
<li>options: Options object (see <tt class="docutils literal"><span class="pre"><span class="Identifier">steepestDescentOptions</span></span></tt> for constructing one)</li>
<li>analyticGradient: The analytic gradient of <tt class="docutils literal"><span class="pre"><span class="Identifier">f</span></span></tt> taking in and returning a 1D Tensor. If not provided, a finite difference approximation will be performed instead.</li>
</ul>
<p>Returns:</p>
<ul class="simple"><li>The final solution for the parameters. Either because a (local) minimum was found or because the maximum number of iterations was reached.</li>
</ul>

&nbsp;&nbsp;<a
href="https://github.com/SciNim/numericalnim/tree/master/src/numericalnim/optimize.nim#L352"
class="link-seesrc" target="_blank">Source</a>
&nbsp;&nbsp;<a href="https://github.com/SciNim/numericalnim/edit/devel/src/numericalnim/optimize.nim#L352" class="link-seesrc" target="_blank" >Edit</a>

</dd>
</div>
<div id="steepestDescentOptions,U,U,bool,int,LineSearchCriterion">
<dt><pre><span class="Keyword">proc</span> <a href="#steepestDescentOptions%2CU%2CU%2Cbool%2Cint%2CLineSearchCriterion"><span class="Identifier">steepestDescentOptions</span></a><span class="Other">[</span><span class="Identifier">U</span><span class="Other">]</span><span class="Other">(</span><span class="Identifier">tol</span><span class="Other">:</span> <span class="Identifier">U</span> <span class="Other">=</span> <span class="Identifier">U</span><span class="Other">(</span><span class="FloatNumber">0.000001</span><span class="Other">)</span><span class="Other">;</span> <span class="Identifier">alpha</span><span class="Other">:</span> <span class="Identifier">U</span> <span class="Other">=</span> <span class="Identifier">U</span><span class="Other">(</span><span class="FloatNumber">0.001</span><span class="Other">)</span><span class="Other">;</span>
                               <span class="Identifier">fastMode</span><span class="Other">:</span> <span class="Identifier">bool</span> <span class="Other">=</span> <span class="Identifier">false</span><span class="Other">;</span>
                               <span class="Identifier">maxIterations</span><span class="Other">:</span> <span class="Identifier">int</span> <span class="Other">=</span> <span class="DecNumber">10000</span><span class="Other">;</span> <span class="Identifier">lineSearchCriterion</span><span class="Other">:</span> <a href="optimize.html#LineSearchCriterion"><span class="Identifier">LineSearchCriterion</span></a> <span class="Other">=</span> <span class="Identifier">NoLineSearch</span><span class="Other">)</span><span class="Other">:</span> <a href="optimize.html#OptimOptions"><span class="Identifier">OptimOptions</span></a><span class="Other">[</span>
    <span class="Identifier">U</span><span class="Other">,</span> <a href="optimize.html#StandardOptions"><span class="Identifier">StandardOptions</span></a><span class="Other">]</span></pre></dt>
<dd>

Returns a Steepest Descent OptimOptions<ul class="simple"><li>tol: The tolerance used. This is the criteria for convergence: <tt class="docutils literal"><span class="pre"><span class="Identifier">gradNorm</span> <span class="Operator">&lt;</span> <span class="Identifier">tol</span><span class="Operator">*</span><span class="Punctuation">(</span><span class="DecNumber">1</span> <span class="Operator">+</span> <span class="Identifier">fNorm</span><span class="Punctuation">)</span></span></tt>.</li>
<li>alpha: The step size.</li>
<li>fastMode: If true, a faster first order accurate finite difference approximation of the derivative will be used. Else a more accurate but slowe second order finite difference scheme will be used.</li>
<li>maxIteration: The maximum number of iteration before returning if convergence haven't been reached.</li>
<li>lineSearchCriterion: Which line search method to use.</li>
</ul>

&nbsp;&nbsp;<a
href="https://github.com/SciNim/numericalnim/tree/master/src/numericalnim/optimize.nim#L217"
class="link-seesrc" target="_blank">Source</a>
&nbsp;&nbsp;<a href="https://github.com/SciNim/numericalnim/edit/devel/src/numericalnim/optimize.nim#L217" class="link-seesrc" target="_blank" >Edit</a>

</dd>
</div>
<div id="vectorNorm,Tensor[T]">
<dt><pre><span class="Keyword">proc</span> <a href="#vectorNorm%2CTensor%5BT%5D"><span class="Identifier">vectorNorm</span></a><span class="Other">[</span><span class="Identifier">T</span><span class="Other">]</span><span class="Other">(</span><span class="Identifier">v</span><span class="Other">:</span> <span class="Identifier">Tensor</span><span class="Other">[</span><span class="Identifier">T</span><span class="Other">]</span><span class="Other">)</span><span class="Other">:</span> <span class="Identifier">T</span></pre></dt>
<dd>

Calculates the norm of the vector, ie the sqrt(Σ vᵢ²)
&nbsp;&nbsp;<a
href="https://github.com/SciNim/numericalnim/tree/master/src/numericalnim/optimize.nim#L294"
class="link-seesrc" target="_blank">Source</a>
&nbsp;&nbsp;<a href="https://github.com/SciNim/numericalnim/edit/devel/src/numericalnim/optimize.nim#L294" class="link-seesrc" target="_blank" >Edit</a>

</dd>
</div>

</dl></div>

  </div>
</div>

    <div class="row">
      <div class="twelve-columns footer">
        <span class="nim-sprite"></span>
        <br/>
        <small style="color: var(--hint);">Made with Nim. Generated: 2023-07-07 09:38:41 UTC</small>
      </div>
    </div>
  </div>
</div>

</body>
</html>
